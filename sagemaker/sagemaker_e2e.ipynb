{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a7d2b8",
   "metadata": {},
   "source": [
    "# AWS SageMaker Customer Churn Prediction\n",
    "\n",
    "This notebook demonstrates how to train and deploy a machine learning model for customer churn prediction using Amazon SageMaker.\n",
    "\n",
    "## Overview\n",
    "- Train a Random Forest model using SageMaker's SKLearn estimator\n",
    "- Deploy the model to a SageMaker endpoint for real-time inference\n",
    "- Handle both JSON and CSV input formats for predictions\n",
    "\n",
    "## Prerequisites\n",
    "- AWS account with SageMaker access\n",
    "- Proper IAM roles configured\n",
    "- S3 bucket for storing data and model artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a7fbd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "befaf853",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a9c8ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.sklearn import SKLearn, SKLearnModel\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize SageMaker session\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"Default S3 bucket: {bucket}\")\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b441ded2",
   "metadata": {},
   "source": [
    "## 2. Upload Data to S3 Bucket\n",
    "\n",
    "Before training, we need to upload our datasets to S3 so SageMaker can access them during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec7273",
   "metadata": {},
   "source": [
    "# Verify inference script exists\n",
    "inference_script_path = 'inference.py'\n",
    "if not os.path.exists(inference_script_path):\n",
    "    print(f\"Error: {inference_script_path} not found in current directory\")\n",
    "    print(\"Available files:\", os.listdir('.'))\n",
    "else:\n",
    "    print(f\"Found inference script: {inference_script_path}\")\n",
    "\n",
    "# Create SageMaker model using the actual inference script\n",
    "model = SKLearnModel(\n",
    "    model_data=sklearn_estimator.model_data,\n",
    "    role=role,\n",
    "    entry_point='inference.py',\n",
    "    source_dir='.',\n",
    "    framework_version='1.0-1',\n",
    "    py_version='py3'\n",
    ")\n",
    "\n",
    "print(\"SageMaker model created successfully with enhanced inference script\")\n",
    "\n",
    "# Deploy to endpoint\n",
    "endpoint_name = 'customer-churn-endpoint'\n",
    "print(f\"Deploying model to endpoint: {endpoint_name}\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "try:\n",
    "    predictor = model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type='ml.t2.medium',\n",
    "        endpoint_name=endpoint_name\n",
    "    )\n",
    "    \n",
    "    print(f\"Model successfully deployed to endpoint: {endpoint_name}\")\n",
    "    print(f\"Endpoint name: {predictor.endpoint_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Deployment error: {e}\")\n",
    "    print(\"Note: If endpoint already exists, you may need to delete it first or use a different name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f31fb1",
   "metadata": {},
   "source": [
    "## 3. Model Training with SageMaker\n",
    "\n",
    "We'll use our existing training script and SageMaker's SKLearn estimator to train our Random Forest model using the data we uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d4b6dc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Upload the training script to S3\n",
    "import os\n",
    "\n",
    "# Verify training script exists\n",
    "training_script_path = 'training.py'\n",
    "if not os.path.exists(training_script_path):\n",
    "    print(f\"‚ùå Error: {training_script_path} not found in current directory\")\n",
    "    print(\"Available files:\", os.listdir('.'))\n",
    "else:\n",
    "    print(f\"‚úÖ Found training script: {training_script_path}\")\n",
    "    \n",
    "    # Upload training script to S3\n",
    "    s3_key = 'code/training.py'\n",
    "    s3.upload_file(training_script_path, bucket, s3_key)\n",
    "    print(f\"üì§ Training script uploaded to s3://{bucket}/{s3_key}\")\n",
    "\n",
    "# Create SKLearn estimator with the actual training script\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point='training.py',\n",
    "    source_dir='.',\n",
    "    role=role,\n",
    "    instance_type='ml.m5.large',\n",
    "    framework_version='1.0-1',\n",
    "    py_version='py3',\n",
    "    hyperparameters={\n",
    "        'n-estimators': 100,\n",
    "        'max-depth': 10,\n",
    "        'min-samples-split': 2,\n",
    "        'min-samples-leaf': 1,\n",
    "        'random-state': 42\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"ü§ñ SKLearn estimator created successfully with enhanced training script\")\n",
    "\n",
    "# Define training data location (using the processed data uploaded in section 2)\n",
    "train_input = TrainingInput(f's3://{bucket}/data/processed/', content_type='text/csv')\n",
    "\n",
    "print(f\"üìä Training data location: s3://{bucket}/data/processed/\")\n",
    "print(\"üöÄ Starting training job...\")\n",
    "\n",
    "# Start training job\n",
    "sklearn_estimator.fit({'train': train_input})\n",
    "print(\"‚úÖ Training job completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f47694",
   "metadata": {},
   "source": [
    "## 4. Model Deployment and Inference\n",
    "\n",
    "Now we'll use our existing inference script and deploy the trained model to a SageMaker endpoint for real-time predictions. The inference script includes:\n",
    "\n",
    "- Enhanced error handling and logging\n",
    "- Support for both JSON and CSV input formats\n",
    "- Detailed prediction results with confidence scores\n",
    "- Health check functionality\n",
    "- Batch prediction support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4fc455",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38482ae0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Verify inference script exists\n",
    "inference_script_path = 'inference.py'\n",
    "if not os.path.exists(inference_script_path):\n",
    "    print(f\"Error: {inference_script_path} not found in current directory\")\n",
    "    print(\"Available files:\", os.listdir('.'))\n",
    "else:\n",
    "    print(f\"Found inference script: {inference_script_path}\")\n",
    "\n",
    "# Create SageMaker model using the actual inference script\n",
    "model = SKLearnModel(\n",
    "    model_data=sklearn_estimator.model_data,\n",
    "    role=role,\n",
    "    entry_point='inference.py',\n",
    "    source_dir='.',\n",
    "    framework_version='1.0-1',\n",
    "    py_version='py3'\n",
    ")\n",
    "\n",
    "print(\"SageMaker model created successfully with enhanced inference script\")\n",
    "\n",
    "# Deploy to endpoint\n",
    "endpoint_name = 'customer-churn-endpoint'\n",
    "print(f\"Deploying model to endpoint: {endpoint_name}\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "try:\n",
    "    predictor = model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type='ml.t2.medium',\n",
    "        endpoint_name=endpoint_name\n",
    "    )\n",
    "    \n",
    "    print(f\"Model successfully deployed to endpoint: {endpoint_name}\")\n",
    "    print(f\"Endpoint name: {predictor.endpoint_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Deployment error: {e}\")\n",
    "    print(\"Note: If endpoint already exists, you may need to delete it first or use a different name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6873c84",
   "metadata": {},
   "source": [
    "## 5. Testing the Endpoint\n",
    "\n",
    "Let's test our deployed endpoint with sample data using the uploaded processed dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1411d18a",
   "metadata": {},
   "source": [
    "# Test the Endpoint with Correct Format\n",
    "\n",
    "print(\"üß™ Testing endpoint with correct format...\")\n",
    "\n",
    "# Load the data to understand structure\n",
    "try:\n",
    "    df = pd.read_csv('../data/customer_churn_processed.csv')\n",
    "    if 'Churn' in df.columns:\n",
    "        features_df = df.drop('Churn', axis=1)\n",
    "        print(f\"üìä Model expects {len(features_df.columns)} features\")\n",
    "        \n",
    "        # Get sample data\n",
    "        sample_data = features_df.iloc[0].to_dict()\n",
    "        \n",
    "        # Convert numpy types to Python types\n",
    "        for key, value in sample_data.items():\n",
    "            if hasattr(value, 'item'):\n",
    "                sample_data[key] = value.item()\n",
    "        \n",
    "        print(f\"üìù Testing with {len(sample_data)} features...\")\n",
    "        \n",
    "        # Test 1: JSON format (most reliable for SageMaker)\n",
    "        print(\"\\nüß™ Test 1: JSON with proper serialization\")\n",
    "        try:\n",
    "            # Use the predictor's built-in serialization\n",
    "            from sagemaker.serializers import JSONSerializer\n",
    "            from sagemaker.deserializers import JSONDeserializer\n",
    "            \n",
    "            # Set serializers\n",
    "            predictor.serializer = JSONSerializer()\n",
    "            predictor.deserializer = JSONDeserializer()\n",
    "            \n",
    "            result = predictor.predict(sample_data)\n",
    "            print(\"‚úÖ JSON test successful!\")\n",
    "            print(f\"üéØ Prediction: {result}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå JSON test failed: {e}\")\n",
    "            \n",
    "            # Test 2: Manual JSON approach\n",
    "            print(\"\\nüß™ Test 2: Manual JSON formatting\")\n",
    "            try:\n",
    "                import json\n",
    "                json_data = json.dumps(sample_data)\n",
    "                print(f\"üì§ Sending JSON: {json_data[:100]}...\")  # Show first 100 chars\n",
    "                \n",
    "                result = predictor.predict(\n",
    "                    json_data,\n",
    "                    initial_args={'ContentType': 'application/json'}\n",
    "                )\n",
    "                print(\"‚úÖ Manual JSON test successful!\")\n",
    "                print(f\"üéØ Result: {result}\")\n",
    "                \n",
    "            except Exception as e2:\n",
    "                print(f\"‚ùå Manual JSON failed: {e2}\")\n",
    "                \n",
    "                # Test 3: Use SageMaker CSV serializer\n",
    "                print(\"\\nüß™ Test 3: CSV with SageMaker serializer\")\n",
    "                try:\n",
    "                    from sagemaker.serializers import CSVSerializer\n",
    "                    \n",
    "                    predictor.serializer = CSVSerializer()\n",
    "                    predictor.deserializer = JSONDeserializer()\n",
    "                    \n",
    "                    # Convert to list of values (no headers)\n",
    "                    values = list(sample_data.values())\n",
    "                    result = predictor.predict([values])  # Wrap in list for CSV\n",
    "                    \n",
    "                    print(\"‚úÖ CSV serializer test successful!\")\n",
    "                    print(f\"üéØ Result: {result}\")\n",
    "                    \n",
    "                except Exception as e3:\n",
    "                    print(f\"‚ùå CSV serializer failed: {e3}\")\n",
    "                    print(\"\\nÔøΩ Debug info:\")\n",
    "                    print(f\"   - Sample data keys: {list(sample_data.keys())[:5]}...\")\n",
    "                    print(f\"   - Sample data values: {list(sample_data.values())[:5]}...\")\n",
    "                    print(f\"   - Data types: {[type(v).__name__ for v in list(sample_data.values())[:5]]}\")\n",
    "    else:\n",
    "        print(\"‚ùå No 'Churn' column found in data\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Training data file not found\")\n",
    "    print(\"üí° Make sure '../data/customer_churn_processed.csv' exists\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "\n",
    "print(\"\\nüéâ Testing completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809c00d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Debug: Understanding the CSV Parsing Issue\n",
    "\n",
    "print(\"üîç Diagnosing the CSV parsing issue...\")\n",
    "\n",
    "# The error shows: \"Expected 4 fields in line 2, saw 12\"\n",
    "# This means your inference script expects CSV with only 4 columns\n",
    "# But your data has many more columns\n",
    "\n",
    "try:\n",
    "    # Check your actual data structure\n",
    "    df = pd.read_csv('../data/customer_churn_processed.csv')\n",
    "    if 'Churn' in df.columns:\n",
    "        features_df = df.drop('Churn', axis=1)\n",
    "        print(f\"üìä Your data has {len(features_df.columns)} features\")\n",
    "        print(f\"üìã First 10 features: {list(features_df.columns)[:10]}\")\n",
    "        \n",
    "        # Show what CSV data looks like\n",
    "        sample_row = features_df.iloc[0]\n",
    "        csv_string = ','.join([str(val) for val in sample_row.values])\n",
    "        print(f\"\\nüìù CSV string would be:\")\n",
    "        print(f\"   {csv_string[:100]}...\")  # First 100 chars\n",
    "        print(f\"   Total fields: {len(sample_row)}\")\n",
    "        \n",
    "    # Check inference script CSV handling\n",
    "    print(f\"\\nüîç Checking inference.py CSV handling...\")\n",
    "    with open('inference.py', 'r') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    # Look for CSV parsing\n",
    "    lines = content.split('\\n')\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'read_csv' in line and 'header=None' in line:\n",
    "            print(f\"   Line {i+1}: {line.strip()}\")\n",
    "            # Show context\n",
    "            for j in range(max(0, i-2), min(len(lines), i+3)):\n",
    "                if j != i:\n",
    "                    print(f\"   Line {j+1}: {lines[j].strip()}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nüí° SOLUTION:\")\n",
    "    print(f\"   The issue is that your inference script's CSV parser\")\n",
    "    print(f\"   expects a specific number of columns, but your data has {len(features_df.columns)}\")\n",
    "    print(f\"   We need to use JSON format instead, which is more flexible\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in diagnosis: {e}\")\n",
    "\n",
    "print(f\"\\nüéØ RECOMMENDATION: Use JSON format with proper SageMaker serializers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbeb18e",
   "metadata": {},
   "source": [
    "## 6. Cleanup\n",
    "\n",
    "Remember to delete the endpoint when you're done to avoid ongoing charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbd6829",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup Resources\n",
    "# Remember to delete the endpoint when you're done to avoid ongoing charges\n",
    "\n",
    "import boto3\n",
    "\n",
    "print(f\"Current endpoint: {endpoint_name}\")\n",
    "\n",
    "# Option 1: Delete just the endpoint (keeps the model)\n",
    "def delete_endpoint():\n",
    "    try:\n",
    "        predictor.delete_endpoint()\n",
    "        print(f\"Endpoint '{endpoint_name}' deleted successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting endpoint: {e}\")\n",
    "\n",
    "# Option 2: Delete endpoint and model\n",
    "def delete_endpoint_and_model():\n",
    "    try:\n",
    "        predictor.delete_endpoint()\n",
    "        predictor.delete_model()\n",
    "        print(f\"Endpoint '{endpoint_name}' and model deleted successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting endpoint and model: {e}\")\n",
    "\n",
    "# Option 3: List all endpoints to see what's running\n",
    "def list_endpoints():\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    try:\n",
    "        response = sm_client.list_endpoints()\n",
    "        endpoints = response['Endpoints']\n",
    "        \n",
    "        if endpoints:\n",
    "            print(\"Active endpoints:\")\n",
    "            for ep in endpoints:\n",
    "                print(f\"- {ep['EndpointName']} (Status: {ep['EndpointStatus']})\")\n",
    "        else:\n",
    "            print(\"No active endpoints found\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing endpoints: {e}\")\n",
    "\n",
    "# Show current status\n",
    "print(\"\\nEndpoint Management Options:\")\n",
    "print(\"1. delete_endpoint() - Delete only the endpoint\")\n",
    "print(\"2. delete_endpoint_and_model() - Delete endpoint and model\")\n",
    "print(\"3. list_endpoints() - List all active endpoints\")\n",
    "\n",
    "# Uncomment the line below when you want to delete the endpoint\n",
    "# delete_endpoint()\n",
    "\n",
    "# Show active endpoints\n",
    "list_endpoints()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
