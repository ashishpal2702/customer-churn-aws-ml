{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e86d6e05",
   "metadata": {},
   "source": [
    "# AWS SageMaker Customer Churn Prediction\n",
    "\n",
    "This notebook demonstrates how to train and deploy a machine learning model for customer churn prediction using Amazon SageMaker.\n",
    "\n",
    "## Overview\n",
    "- Train a Random Forest model using SageMaker's SKLearn estimator\n",
    "- Deploy the model to a SageMaker endpoint for real-time inference\n",
    "- Handle both JSON and CSV input formats for predictions\n",
    "\n",
    "## Prerequisites\n",
    "- AWS account with SageMaker access\n",
    "- Proper IAM roles configured\n",
    "- S3 bucket for storing data and model artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709cc92e",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9374df68",
   "metadata": {},
   "source": [
    "## Enhanced Training and Inference Scripts\n",
    "\n",
    "This notebook uses production-ready training and inference scripts with the following improvements:\n",
    "\n",
    "### Training Script (`training.py`) Features:\n",
    "- **Robust data loading**: Automatically finds and loads CSV files in the training directory\n",
    "- **Enhanced logging**: Comprehensive logging for debugging and monitoring\n",
    "- **Model validation**: Built-in train/validation split with performance metrics\n",
    "- **Feature importance analysis**: Automatically reports most important features\n",
    "- **Flexible hyperparameters**: Support for all Random Forest parameters\n",
    "- **Error handling**: Graceful error handling with informative messages\n",
    "\n",
    "### Inference Script (`inference.py`) Features:\n",
    "- **Multiple input formats**: Supports both JSON and CSV input\n",
    "- **Batch predictions**: Can handle single instances or batches\n",
    "- **Enhanced outputs**: Returns predictions, probabilities, and confidence scores\n",
    "- **Robust error handling**: Comprehensive error handling and logging\n",
    "- **Health checks**: Built-in health check functionality\n",
    "- **Production ready**: Follows SageMaker best practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fa260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.sklearn import SKLearn, SKLearnModel\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize SageMaker session\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"Default S3 bucket: {bucket}\")\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5842ea09",
   "metadata": {},
   "source": [
    "## 2. Upload Data to S3 Bucket\n",
    "\n",
    "Before training, we need to upload our datasets to S3 so SageMaker can access them during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7526c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def upload_data_to_s3(local_path, s3_key, description=\"\"):\n",
    "    \"\"\"Upload a file to S3 with error handling and verification.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(local_path):\n",
    "            print(f\"‚ùå File not found: {local_path}\")\n",
    "            return False\n",
    "            \n",
    "        # Get file size for progress indication\n",
    "        file_size = os.path.getsize(local_path) / (1024 * 1024)  # MB\n",
    "        print(f\"üìÅ Uploading {description}: {local_path}\")\n",
    "        print(f\"   Size: {file_size:.2f} MB\")\n",
    "        \n",
    "        # Upload file\n",
    "        s3.upload_file(local_path, bucket, s3_key)\n",
    "        print(f\"‚úÖ Successfully uploaded to: s3://{bucket}/{s3_key}\")\n",
    "        \n",
    "        # Verify upload\n",
    "        response = s3.head_object(Bucket=bucket, Key=s3_key)\n",
    "        print(f\"   Verified: {response['ContentLength']} bytes\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error uploading {local_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def show_data_info(file_path):\n",
    "    \"\"\"Display basic information about the dataset.\"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"   üìä Shape: {df.shape}\")\n",
    "            print(f\"   üìã Columns: {list(df.columns)[:5]}{'...' if len(df.columns) > 5 else ''}\")\n",
    "            if 'Churn' in df.columns:\n",
    "                print(f\"   üéØ Target distribution: {df['Churn'].value_counts().to_dict()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Could not read data info: {e}\")\n",
    "\n",
    "print(\"üöÄ Starting data upload to S3...\")\n",
    "print(f\"üì¶ S3 Bucket: {bucket}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define data files to upload\n",
    "data_files = [\n",
    "    {\n",
    "        'local_path': '../data/customer_churn.csv',\n",
    "        's3_key': 'data/raw/customer_churn.csv',\n",
    "        'description': 'Raw customer churn dataset'\n",
    "    },\n",
    "    {\n",
    "        'local_path': '../data/customer_churn_processed.csv',\n",
    "        's3_key': 'data/processed/customer_churn_processed.csv',\n",
    "        'description': 'Processed customer churn dataset (for training)'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Upload each data file\n",
    "upload_results = []\n",
    "for data_file in data_files:\n",
    "    print(f\"\\nüì§ Processing: {data_file['description']}\")\n",
    "    \n",
    "    # Show data info before upload\n",
    "    if os.path.exists(data_file['local_path']):\n",
    "        show_data_info(data_file['local_path'])\n",
    "    \n",
    "    # Upload file\n",
    "    success = upload_data_to_s3(\n",
    "        data_file['local_path'], \n",
    "        data_file['s3_key'], \n",
    "        data_file['description']\n",
    "    )\n",
    "    upload_results.append({\n",
    "        'file': data_file['description'],\n",
    "        'success': success,\n",
    "        's3_path': f\"s3://{bucket}/{data_file['s3_key']}\" if success else None\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã UPLOAD SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "successful_uploads = 0\n",
    "for result in upload_results:\n",
    "    status = \"‚úÖ SUCCESS\" if result['success'] else \"‚ùå FAILED\"\n",
    "    print(f\"{status}: {result['file']}\")\n",
    "    if result['success']:\n",
    "        successful_uploads += 1\n",
    "        print(f\"         S3 Path: {result['s3_path']}\")\n",
    "\n",
    "print(f\"\\nüìä Total: {successful_uploads}/{len(upload_results)} files uploaded successfully\")\n",
    "\n",
    "# List S3 contents to verify\n",
    "print(f\"\\nüîç Verifying S3 bucket contents...\")\n",
    "try:\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix='data/')\n",
    "    if 'Contents' in response:\n",
    "        print(f\"üìÅ Files in s3://{bucket}/data/:\")\n",
    "        for obj in response['Contents']:\n",
    "            size_mb = obj['Size'] / (1024 * 1024)\n",
    "            mod_time = obj['LastModified'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(f\"   üìÑ {obj['Key']} ({size_mb:.2f} MB, {mod_time})\")\n",
    "    else:\n",
    "        print(\"   üì≠ No files found in data/ prefix\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Error listing S3 contents: {e}\")\n",
    "\n",
    "print(f\"\\nüéâ Data upload process completed!\")\n",
    "print(f\"üí° Use 's3://{bucket}/data/processed/' as your training data input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b447e7",
   "metadata": {},
   "source": [
    "## 3. Model Training with SageMaker\n",
    "\n",
    "We'll use our existing training script and SageMaker's SKLearn estimator to train our Random Forest model using the data we uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ec607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the training script to S3\n",
    "import os\n",
    "\n",
    "# Verify training script exists\n",
    "training_script_path = 'training.py'\n",
    "if not os.path.exists(training_script_path):\n",
    "    print(f\"‚ùå Error: {training_script_path} not found in current directory\")\n",
    "    print(\"Available files:\", os.listdir('.'))\n",
    "else:\n",
    "    print(f\"‚úÖ Found training script: {training_script_path}\")\n",
    "    \n",
    "    # Upload training script to S3\n",
    "    s3_key = 'code/training.py'\n",
    "    s3.upload_file(training_script_path, bucket, s3_key)\n",
    "    print(f\"üì§ Training script uploaded to s3://{bucket}/{s3_key}\")\n",
    "\n",
    "# Create SKLearn estimator with the actual training script\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point='training.py',\n",
    "    source_dir='.',\n",
    "    role=role,\n",
    "    instance_type='ml.m5.large',\n",
    "    framework_version='1.0-1',\n",
    "    py_version='py3',\n",
    "    hyperparameters={\n",
    "        'n-estimators': 100,\n",
    "        'max-depth': 10,\n",
    "        'min-samples-split': 2,\n",
    "        'min-samples-leaf': 1,\n",
    "        'random-state': 42\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"ü§ñ SKLearn estimator created successfully with enhanced training script\")\n",
    "\n",
    "# Define training data location (using the processed data uploaded in section 2)\n",
    "train_input = TrainingInput(f's3://{bucket}/data/processed/', content_type='text/csv')\n",
    "\n",
    "print(f\"üìä Training data location: s3://{bucket}/data/processed/\")\n",
    "print(\"üöÄ Starting training job...\")\n",
    "\n",
    "# Start training job\n",
    "sklearn_estimator.fit({'train': train_input})\n",
    "print(\"‚úÖ Training job completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468137c",
   "metadata": {},
   "source": [
    "## 4. Model Deployment and Inference\n",
    "\n",
    "Now we'll use our existing inference script and deploy the trained model to a SageMaker endpoint for real-time predictions. The inference script includes:\n",
    "\n",
    "- Enhanced error handling and logging\n",
    "- Support for both JSON and CSV input formats\n",
    "- Detailed prediction results with confidence scores\n",
    "- Health check functionality\n",
    "- Batch prediction support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9e7a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify inference script exists\n",
    "inference_script_path = 'inference.py'\n",
    "if not os.path.exists(inference_script_path):\n",
    "    print(f\"Error: {inference_script_path} not found in current directory\")\n",
    "    print(\"Available files:\", os.listdir('.'))\n",
    "else:\n",
    "    print(f\"Found inference script: {inference_script_path}\")\n",
    "\n",
    "# Create SageMaker model using the actual inference script\n",
    "model = SKLearnModel(\n",
    "    model_data=sklearn_estimator.model_data,\n",
    "    role=role,\n",
    "    entry_point='inference.py',\n",
    "    source_dir='.',\n",
    "    framework_version='1.0-1',\n",
    "    py_version='py3'\n",
    ")\n",
    "\n",
    "print(\"SageMaker model created successfully with enhanced inference script\")\n",
    "\n",
    "# Deploy to endpoint\n",
    "endpoint_name = 'customer-churn-endpoint'\n",
    "print(f\"Deploying model to endpoint: {endpoint_name}\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "try:\n",
    "    predictor = model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type='ml.t2.medium',\n",
    "        endpoint_name=endpoint_name\n",
    "    )\n",
    "    \n",
    "    print(f\"Model successfully deployed to endpoint: {endpoint_name}\")\n",
    "    print(f\"Endpoint name: {predictor.endpoint_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Deployment error: {e}\")\n",
    "    print(\"Note: If endpoint already exists, you may need to delete it first or use a different name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5396c6a5",
   "metadata": {},
   "source": [
    "## 5. Testing the Endpoint\n",
    "\n",
    "Let's test our deployed endpoint with sample data using the uploaded processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Endpoint with Correct Format\n",
    "\n",
    "print(\"üß™ Testing endpoint with correct format...\")\n",
    "\n",
    "# Load the data to understand structure\n",
    "try:\n",
    "    df = pd.read_csv('../data/customer_churn_processed.csv')\n",
    "    if 'Churn' in df.columns:\n",
    "        features_df = df.drop('Churn', axis=1)\n",
    "        print(f\"üìä Model expects {len(features_df.columns)} features\")\n",
    "        \n",
    "        # Get sample data\n",
    "        sample_data = features_df.iloc[0].to_dict()\n",
    "        \n",
    "        # Convert numpy types to Python types\n",
    "        for key, value in sample_data.items():\n",
    "            if hasattr(value, 'item'):\n",
    "                sample_data[key] = value.item()\n",
    "        \n",
    "        print(f\"üìù Testing with {len(sample_data)} features...\")\n",
    "        \n",
    "        # Test 1: JSON format (most reliable for SageMaker)\n",
    "        print(\"\\nüß™ Test 1: JSON with proper serialization\")\n",
    "        try:\n",
    "            # Use the predictor's built-in serialization\n",
    "            from sagemaker.serializers import JSONSerializer\n",
    "            from sagemaker.deserializers import JSONDeserializer\n",
    "            \n",
    "            # Set serializers\n",
    "            predictor.serializer = JSONSerializer()\n",
    "            predictor.deserializer = JSONDeserializer()\n",
    "            \n",
    "            result = predictor.predict(sample_data)\n",
    "            print(\"‚úÖ JSON test successful!\")\n",
    "            print(f\"üéØ Prediction: {result}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå JSON test failed: {e}\")\n",
    "            \n",
    "            # Test 2: Manual JSON approach\n",
    "            print(\"\\nüß™ Test 2: Manual JSON formatting\")\n",
    "            try:\n",
    "                import json\n",
    "                json_data = json.dumps(sample_data)\n",
    "                print(f\"üì§ Sending JSON: {json_data[:100]}...\")  # Show first 100 chars\n",
    "                \n",
    "                result = predictor.predict(\n",
    "                    json_data,\n",
    "                    initial_args={'ContentType': 'application/json'}\n",
    "                )\n",
    "                print(\"‚úÖ Manual JSON test successful!\")\n",
    "                print(f\"üéØ Result: {result}\")\n",
    "                \n",
    "            except Exception as e2:\n",
    "                print(f\"‚ùå Manual JSON failed: {e2}\")\n",
    "                \n",
    "                # Test 3: Use SageMaker CSV serializer\n",
    "                print(\"\\nüß™ Test 3: CSV with SageMaker serializer\")\n",
    "                try:\n",
    "                    from sagemaker.serializers import CSVSerializer\n",
    "                    \n",
    "                    predictor.serializer = CSVSerializer()\n",
    "                    predictor.deserializer = JSONDeserializer()\n",
    "                    \n",
    "                    # Convert to list of values (no headers)\n",
    "                    values = list(sample_data.values())\n",
    "                    result = predictor.predict([values])  # Wrap in list for CSV\n",
    "                    \n",
    "                    print(\"‚úÖ CSV serializer test successful!\")\n",
    "                    print(f\"üéØ Result: {result}\")\n",
    "                    \n",
    "                except Exception as e3:\n",
    "                    print(f\"‚ùå CSV serializer failed: {e3}\")\n",
    "                    print(\"\\n\udd0d Debug info:\")\n",
    "                    print(f\"   - Sample data keys: {list(sample_data.keys())[:5]}...\")\n",
    "                    print(f\"   - Sample data values: {list(sample_data.values())[:5]}...\")\n",
    "                    print(f\"   - Data types: {[type(v).__name__ for v in list(sample_data.values())[:5]]}\")\n",
    "    else:\n",
    "        print(\"‚ùå No 'Churn' column found in data\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Training data file not found\")\n",
    "    print(\"üí° Make sure '../data/customer_churn_processed.csv' exists\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "\n",
    "print(\"\\nüéâ Testing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fce9b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Understanding the CSV Parsing Issue\n",
    "\n",
    "print(\"üîç Diagnosing the CSV parsing issue...\")\n",
    "\n",
    "# The error shows: \"Expected 4 fields in line 2, saw 12\"\n",
    "# This means your inference script expects CSV with only 4 columns\n",
    "# But your data has many more columns\n",
    "\n",
    "try:\n",
    "    # Check your actual data structure\n",
    "    df = pd.read_csv('../data/customer_churn_processed.csv')\n",
    "    if 'Churn' in df.columns:\n",
    "        features_df = df.drop('Churn', axis=1)\n",
    "        print(f\"üìä Your data has {len(features_df.columns)} features\")\n",
    "        print(f\"üìã First 10 features: {list(features_df.columns)[:10]}\")\n",
    "        \n",
    "        # Show what CSV data looks like\n",
    "        sample_row = features_df.iloc[0]\n",
    "        csv_string = ','.join([str(val) for val in sample_row.values])\n",
    "        print(f\"\\nüìù CSV string would be:\")\n",
    "        print(f\"   {csv_string[:100]}...\")  # First 100 chars\n",
    "        print(f\"   Total fields: {len(sample_row)}\")\n",
    "        \n",
    "    # Check inference script CSV handling\n",
    "    print(f\"\\nüîç Checking inference.py CSV handling...\")\n",
    "    with open('inference.py', 'r') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    # Look for CSV parsing\n",
    "    lines = content.split('\\n')\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'read_csv' in line and 'header=None' in line:\n",
    "            print(f\"   Line {i+1}: {line.strip()}\")\n",
    "            # Show context\n",
    "            for j in range(max(0, i-2), min(len(lines), i+3)):\n",
    "                if j != i:\n",
    "                    print(f\"   Line {j+1}: {lines[j].strip()}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nüí° SOLUTION:\")\n",
    "    print(f\"   The issue is that your inference script's CSV parser\")\n",
    "    print(f\"   expects a specific number of columns, but your data has {len(features_df.columns)}\")\n",
    "    print(f\"   We need to use JSON format instead, which is more flexible\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in diagnosis: {e}\")\n",
    "\n",
    "print(f\"\\nüéØ RECOMMENDATION: Use JSON format with proper SageMaker serializers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a667e",
   "metadata": {},
   "source": [
    "## 6. Cleanup\n",
    "\n",
    "Remember to delete the endpoint when you're done to avoid ongoing charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55aef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup Resources\n",
    "# Remember to delete the endpoint when you're done to avoid ongoing charges\n",
    "\n",
    "import boto3\n",
    "\n",
    "print(f\"Current endpoint: {endpoint_name}\")\n",
    "\n",
    "# Option 1: Delete just the endpoint (keeps the model)\n",
    "def delete_endpoint():\n",
    "    try:\n",
    "        predictor.delete_endpoint()\n",
    "        print(f\"Endpoint '{endpoint_name}' deleted successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting endpoint: {e}\")\n",
    "\n",
    "# Option 2: Delete endpoint and model\n",
    "def delete_endpoint_and_model():\n",
    "    try:\n",
    "        predictor.delete_endpoint()\n",
    "        predictor.delete_model()\n",
    "        print(f\"Endpoint '{endpoint_name}' and model deleted successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting endpoint and model: {e}\")\n",
    "\n",
    "# Option 3: List all endpoints to see what's running\n",
    "def list_endpoints():\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    try:\n",
    "        response = sm_client.list_endpoints()\n",
    "        endpoints = response['Endpoints']\n",
    "        \n",
    "        if endpoints:\n",
    "            print(\"Active endpoints:\")\n",
    "            for ep in endpoints:\n",
    "                print(f\"- {ep['EndpointName']} (Status: {ep['EndpointStatus']})\")\n",
    "        else:\n",
    "            print(\"No active endpoints found\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing endpoints: {e}\")\n",
    "\n",
    "# Show current status\n",
    "print(\"\\nEndpoint Management Options:\")\n",
    "print(\"1. delete_endpoint() - Delete only the endpoint\")\n",
    "print(\"2. delete_endpoint_and_model() - Delete endpoint and model\")\n",
    "print(\"3. list_endpoints() - List all active endpoints\")\n",
    "\n",
    "# Uncomment the line below when you want to delete the endpoint\n",
    "# delete_endpoint()\n",
    "\n",
    "# Show active endpoints\n",
    "list_endpoints()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
