{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed38fe21",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction with MLflow Experiment Tracking\n",
    "\n",
    "This notebook demonstrates end-to-end machine learning experiment tracking using MLflow for customer churn prediction.\n",
    "\n",
    "## Features:\n",
    "- Comprehensive experiment tracking\n",
    "- Model comparison and selection\n",
    "- Automated model registry\n",
    "- Reproducible experiments\n",
    "- Detailed performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2380cd",
   "metadata": {},
   "source": [
    "## 1. Setup MLflow Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955bfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MLflow and dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"Successfully installed {package}\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"Failed to install {package}\")\n",
    "\n",
    "# Install required packages\n",
    "packages = ['mlflow', 'xgboost', 'scikit-learn', 'pandas', 'numpy', 'matplotlib', 'seaborn']\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"‚úì {package} is available\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec07084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"./mlruns\")  # Local tracking\n",
    "experiment_name = f\"customer_churn_prediction_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Experiment Name: {experiment_name}\")\n",
    "print(f\"MLflow Version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daee5e4",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow run for data exploration\n",
    "with mlflow.start_run(run_name=\"data_exploration\") as run:\n",
    "    # Load dataset\n",
    "    data_path = \"./data/customer_churn.csv\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Log dataset characteristics\n",
    "    mlflow.log_param(\"dataset_path\", data_path)\n",
    "    mlflow.log_metric(\"total_samples\", len(df))\n",
    "    mlflow.log_metric(\"total_features\", len(df.columns))\n",
    "    mlflow.log_metric(\"missing_values_total\", df.isnull().sum().sum())\n",
    "    \n",
    "    # Target distribution\n",
    "    churn_distribution = df['Churn'].value_counts()\n",
    "    churn_rate = (df['Churn'].sum() / len(df)) * 100\n",
    "    \n",
    "    mlflow.log_metric(\"churn_rate_percent\", churn_rate)\n",
    "    mlflow.log_metric(\"churn_count\", churn_distribution[1.0] if 1.0 in churn_distribution.index else churn_distribution[1])\n",
    "    mlflow.log_metric(\"no_churn_count\", churn_distribution[0.0] if 0.0 in churn_distribution.index else churn_distribution[0])\n",
    "    \n",
    "    # Column information\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    mlflow.log_param(\"numerical_columns\", numerical_cols)\n",
    "    mlflow.log_param(\"categorical_columns\", categorical_cols)\n",
    "    mlflow.log_metric(\"numerical_features_count\", len(numerical_cols))\n",
    "    mlflow.log_metric(\"categorical_features_count\", len(categorical_cols))\n",
    "    \n",
    "    print(f\"\\nDataset info logged to MLflow run: {run.info.run_id}\")\n",
    "    print(f\"Churn rate: {churn_rate:.2f}%\")\n",
    "    print(f\"Numerical columns: {len(numerical_cols)}\")\n",
    "    print(f\"Categorical columns: {len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89643b",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis with MLflow Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa2a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow run for EDA\n",
    "with mlflow.start_run(run_name=\"exploratory_data_analysis\") as run:\n",
    "    # Set style for better plots\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Create comprehensive EDA plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Customer Churn Dataset - Exploratory Data Analysis', fontsize=16)\n",
    "    \n",
    "    # 1. Churn distribution\n",
    "    churn_counts = df['Churn'].value_counts()\n",
    "    axes[0, 0].pie(churn_counts, labels=['No Churn', 'Churn'], autopct='%1.1f%%')\n",
    "    axes[0, 0].set_title('Churn Distribution')\n",
    "    \n",
    "    # 2-6. Feature distributions by churn\n",
    "    features_to_plot = ['Age', 'Total Spend', 'Usage Frequency', 'Support Calls', 'Payment Delay']\n",
    "    positions = [(0, 1), (0, 2), (1, 0), (1, 1), (1, 2)]\n",
    "    \n",
    "    for i, (feature, pos) in enumerate(zip(features_to_plot, positions)):\n",
    "        if feature in df.columns:\n",
    "            sns.boxplot(data=df, x='Churn', y=feature, ax=axes[pos])\n",
    "            axes[pos].set_title(f'{feature} Distribution by Churn')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save and log the plot\n",
    "    eda_plot_path = \"eda_overview.png\"\n",
    "    plt.savefig(eda_plot_path, dpi=300, bbox_inches='tight')\n",
    "    mlflow.log_artifact(eda_plot_path)\n",
    "    plt.show()\n",
    "    \n",
    "    # Categorical variables analysis\n",
    "    categorical_features = ['Gender', 'Subscription Type', 'Contract Length']\n",
    "    available_categorical = [col for col in categorical_features if col in df.columns]\n",
    "    \n",
    "    if available_categorical:\n",
    "        fig, axes = plt.subplots(1, len(available_categorical), figsize=(6*len(available_categorical), 6))\n",
    "        if len(available_categorical) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        fig.suptitle('Categorical Variables vs Churn', fontsize=16)\n",
    "        \n",
    "        for i, col in enumerate(available_categorical):\n",
    "            churn_crosstab = pd.crosstab(df[col], df['Churn'], normalize='index') * 100\n",
    "            churn_crosstab.plot(kind='bar', ax=axes[i], rot=45)\n",
    "            axes[i].set_title(f'{col} vs Churn Rate (%)')\n",
    "            axes[i].set_ylabel('Percentage')\n",
    "            axes[i].legend(['No Churn', 'Churn'])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save and log categorical analysis\n",
    "        categorical_plot_path = \"categorical_analysis.png\"\n",
    "        plt.savefig(categorical_plot_path, dpi=300, bbox_inches='tight')\n",
    "        mlflow.log_artifact(categorical_plot_path)\n",
    "        plt.show()\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    correlation_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=0.1, fmt='.2f')\n",
    "    plt.title('Feature Correlation Heatmap')\n",
    "    \n",
    "    # Save and log correlation heatmap\n",
    "    correlation_plot_path = \"correlation_heatmap.png\"\n",
    "    plt.savefig(correlation_plot_path, dpi=300, bbox_inches='tight')\n",
    "    mlflow.log_artifact(correlation_plot_path)\n",
    "    plt.show()\n",
    "    \n",
    "    # Log correlation statistics\n",
    "    target_correlations = correlation_matrix['Churn'].abs().sort_values(ascending=False)\n",
    "    mlflow.log_metric(\"highest_correlation_with_churn\", target_correlations.iloc[1])  # Excluding self-correlation\n",
    "    mlflow.log_param(\"most_correlated_feature\", target_correlations.index[1])\n",
    "    \n",
    "    print(f\"EDA plots logged to MLflow run: {run.info.run_id}\")\n",
    "    print(f\"Most correlated feature with churn: {target_correlations.index[1]} ({target_correlations.iloc[1]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28df3a2b",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing Pipeline with Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71d83ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Start MLflow run for preprocessing\n",
    "with mlflow.start_run(run_name=\"data_preprocessing\") as run:\n",
    "    def preprocess_data_with_tracking(df):\n",
    "        \"\"\"Comprehensive data preprocessing pipeline with MLflow tracking\"\"\"\n",
    "        \n",
    "        # Log preprocessing parameters\n",
    "        mlflow.log_param(\"preprocessing_strategy\", \"StandardScaler + OneHotEncoder\")\n",
    "        mlflow.log_param(\"missing_value_strategy\", \"drop_rows\")\n",
    "        \n",
    "        # Make a copy and handle missing values\n",
    "        processed_df = df.copy()\n",
    "        initial_shape = processed_df.shape\n",
    "        \n",
    "        # Drop rows with missing values\n",
    "        processed_df = processed_df.dropna()\n",
    "        final_shape = processed_df.shape\n",
    "        \n",
    "        mlflow.log_metric(\"rows_dropped\", initial_shape[0] - final_shape[0])\n",
    "        mlflow.log_metric(\"final_sample_count\", final_shape[0])\n",
    "        \n",
    "        # Remove CustomerID if present\n",
    "        if 'CustomerID' in processed_df.columns:\n",
    "            processed_df = processed_df.drop('CustomerID', axis=1)\n",
    "            mlflow.log_param(\"removed_customer_id\", True)\n",
    "        \n",
    "        # Define column types\n",
    "        numerical_cols = ['Age', 'Tenure', 'Usage Frequency', 'Support Calls', \n",
    "                         'Payment Delay', 'Total Spend', 'Last Interaction']\n",
    "        categorical_cols = ['Gender', 'Subscription Type', 'Contract Length']\n",
    "        target_col = 'Churn'\n",
    "        \n",
    "        # Filter existing columns\n",
    "        numerical_cols = [col for col in numerical_cols if col in processed_df.columns]\n",
    "        categorical_cols = [col for col in categorical_cols if col in processed_df.columns]\n",
    "        \n",
    "        mlflow.log_param(\"numerical_features\", numerical_cols)\n",
    "        mlflow.log_param(\"categorical_features\", categorical_cols)\n",
    "        \n",
    "        # Separate features and target\n",
    "        X = processed_df.drop(target_col, axis=1)\n",
    "        y = processed_df[target_col]\n",
    "        \n",
    "        # Create preprocessing pipeline\n",
    "        numerical_transformer = StandardScaler()\n",
    "        categorical_transformer = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "        \n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numerical_transformer, numerical_cols),\n",
    "                ('cat', categorical_transformer, categorical_cols)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Fit and transform\n",
    "        X_processed = preprocessor.fit_transform(X)\n",
    "        \n",
    "        # Get feature names\n",
    "        try:\n",
    "            categorical_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
    "            all_feature_names = list(numerical_cols) + list(categorical_feature_names)\n",
    "        except:\n",
    "            # Fallback if get_feature_names_out is not available\n",
    "            all_feature_names = list(numerical_cols) + [f\"cat_{i}\" for i in range(X_processed.shape[1] - len(numerical_cols))]\n",
    "        \n",
    "        X_processed_df = pd.DataFrame(X_processed, columns=all_feature_names)\n",
    "        \n",
    "        # Log preprocessing results\n",
    "        mlflow.log_metric(\"original_features\", X.shape[1])\n",
    "        mlflow.log_metric(\"processed_features\", X_processed_df.shape[1])\n",
    "        mlflow.log_metric(\"feature_expansion_ratio\", X_processed_df.shape[1] / X.shape[1])\n",
    "        \n",
    "        # Save preprocessor\n",
    "        preprocessor_path = \"preprocessor.joblib\"\n",
    "        joblib.dump(preprocessor, preprocessor_path)\n",
    "        mlflow.log_artifact(preprocessor_path)\n",
    "        \n",
    "        return X_processed_df, y, preprocessor, all_feature_names\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    X_processed, y, preprocessor, feature_names = preprocess_data_with_tracking(df)\n",
    "    \n",
    "    print(\"Preprocessing completed and logged to MLflow!\")\n",
    "    print(f\"Features shape: {X_processed.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "    print(f\"Feature names: {len(feature_names)} features\")\n",
    "    \n",
    "    # Log target distribution after preprocessing\n",
    "    final_churn_rate = (y.sum() / len(y)) * 100\n",
    "    mlflow.log_metric(\"final_churn_rate_percent\", final_churn_rate)\n",
    "    \n",
    "    print(f\"Final churn rate: {final_churn_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020e30aa",
   "metadata": {},
   "source": [
    "## 5. Model Training with MLflow Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd902181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "# Try to import XGBoost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except ImportError:\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    xgb_available = False\n",
    "    print(\"XGBoost not available, using Decision Tree instead\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "# Define models with hyperparameters\n",
    "models_config = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "        'params': {'n_estimators': 100, 'max_depth': None, 'random_state': 42}\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'params': {'C': 1.0, 'max_iter': 1000, 'random_state': 42}\n",
    "    }\n",
    "}\n",
    "\n",
    "if xgb_available:\n",
    "    models_config['XGBoost'] = {\n",
    "        'model': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        'params': {'n_estimators': 100, 'max_depth': 6, 'random_state': 42}\n",
    "    }\n",
    "else:\n",
    "    models_config['Decision Tree'] = {\n",
    "        'model': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
    "        'params': {'max_depth': 10, 'random_state': 42}\n",
    "    }\n",
    "\n",
    "# Train models with MLflow tracking\n",
    "model_results = {}\n",
    "\n",
    "for model_name, config in models_config.items():\n",
    "    with mlflow.start_run(run_name=f\"model_{model_name.lower().replace(' ', '_')}\") as run:\n",
    "        print(f\"\\nTraining {model_name}...\")\n",
    "        \n",
    "        # Log model parameters\n",
    "        mlflow.log_params(config['params'])\n",
    "        mlflow.log_param(\"model_type\", model_name)\n",
    "        \n",
    "        model = config['model']\n",
    "        \n",
    "        # Training time tracking\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        cv_roc_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "        \n",
    "        # Log cross-validation metrics\n",
    "        mlflow.log_metric(\"cv_accuracy_mean\", cv_scores.mean())\n",
    "        mlflow.log_metric(\"cv_accuracy_std\", cv_scores.std())\n",
    "        mlflow.log_metric(\"cv_roc_auc_mean\", cv_roc_scores.mean())\n",
    "        mlflow.log_metric(\"cv_roc_auc_std\", cv_roc_scores.std())\n",
    "        \n",
    "        # Train final model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        mlflow.log_metric(\"training_time_seconds\", training_time)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        test_accuracy = accuracy_score(y_test, y_pred)\n",
    "        test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        test_precision = precision_score(y_test, y_pred)\n",
    "        test_recall = recall_score(y_test, y_pred)\n",
    "        test_f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        # Log test metrics\n",
    "        mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "        mlflow.log_metric(\"test_roc_auc\", test_roc_auc)\n",
    "        mlflow.log_metric(\"test_precision\", test_precision)\n",
    "        mlflow.log_metric(\"test_recall\", test_recall)\n",
    "        mlflow.log_metric(\"test_f1_score\", test_f1)\n",
    "        \n",
    "        # Log model\n",
    "        if model_name == 'XGBoost' and xgb_available:\n",
    "            mlflow.xgboost.log_model(model, \"model\")\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        # Feature importance (for tree-based models)\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            # Save feature importance plot\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            top_features = feature_importance.head(15)\n",
    "            plt.barh(range(len(top_features)), top_features['importance'])\n",
    "            plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "            plt.xlabel('Importance')\n",
    "            plt.title(f'Top 15 Feature Importances - {model_name}')\n",
    "            plt.gca().invert_yaxis()\n",
    "            \n",
    "            importance_plot_path = f\"feature_importance_{model_name.lower().replace(' ', '_')}.png\"\n",
    "            plt.savefig(importance_plot_path, dpi=300, bbox_inches='tight')\n",
    "            mlflow.log_artifact(importance_plot_path)\n",
    "            plt.show()\n",
    "            \n",
    "            # Log top feature importance\n",
    "            mlflow.log_metric(\"top_feature_importance\", top_features.iloc[0]['importance'])\n",
    "            mlflow.log_param(\"most_important_feature\", top_features.iloc[0]['feature'])\n",
    "        \n",
    "        # Store results\n",
    "        model_results[model_name] = {\n",
    "            'model': model,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'test_roc_auc': test_roc_auc,\n",
    "            'test_precision': test_precision,\n",
    "            'test_recall': test_recall,\n",
    "            'test_f1': test_f1,\n",
    "            'cv_accuracy_mean': cv_scores.mean(),\n",
    "            'cv_roc_auc_mean': cv_roc_scores.mean(),\n",
    "            'run_id': run.info.run_id\n",
    "        }\n",
    "        \n",
    "        print(f\"  CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        print(f\"  CV ROC-AUC: {cv_roc_scores.mean():.4f} (+/- {cv_roc_scores.std() * 2:.4f})\")\n",
    "        print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"  Test ROC-AUC: {test_roc_auc:.4f}\")\n",
    "        print(f\"  Training Time: {training_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nAll models trained and logged to MLflow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7b24af",
   "metadata": {},
   "source": [
    "## 6. Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow run for model comparison\n",
    "with mlflow.start_run(run_name=\"model_comparison\") as run:\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for model_name, results in model_results.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Test Accuracy': results['test_accuracy'],\n",
    "            'Test ROC-AUC': results['test_roc_auc'],\n",
    "            'Test Precision': results['test_precision'],\n",
    "            'Test Recall': results['test_recall'],\n",
    "            'Test F1-Score': results['test_f1'],\n",
    "            'CV Accuracy': results['cv_accuracy_mean'],\n",
    "            'CV ROC-AUC': results['cv_roc_auc_mean']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"Model Comparison:\")\n",
    "    print(comparison_df.round(4))\n",
    "    \n",
    "    # Save comparison table\n",
    "    comparison_path = \"model_comparison.csv\"\n",
    "    comparison_df.to_csv(comparison_path, index=False)\n",
    "    mlflow.log_artifact(comparison_path)\n",
    "    \n",
    "    # Create performance comparison plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Model Performance Comparison', fontsize=16)\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    axes[0, 0].bar(comparison_df['Model'], comparison_df['Test Accuracy'])\n",
    "    axes[0, 0].set_title('Test Accuracy Comparison')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # ROC-AUC comparison\n",
    "    axes[0, 1].bar(comparison_df['Model'], comparison_df['Test ROC-AUC'])\n",
    "    axes[0, 1].set_title('Test ROC-AUC Comparison')\n",
    "    axes[0, 1].set_ylabel('ROC-AUC')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Precision vs Recall\n",
    "    axes[1, 0].scatter(comparison_df['Test Recall'], comparison_df['Test Precision'], s=100)\n",
    "    for i, model in enumerate(comparison_df['Model']):\n",
    "        axes[1, 0].annotate(model, \n",
    "                           (comparison_df['Test Recall'].iloc[i], comparison_df['Test Precision'].iloc[i]),\n",
    "                           xytext=(5, 5), textcoords='offset points')\n",
    "    axes[1, 0].set_xlabel('Recall')\n",
    "    axes[1, 0].set_ylabel('Precision')\n",
    "    axes[1, 0].set_title('Precision vs Recall')\n",
    "    \n",
    "    # F1-Score comparison\n",
    "    axes[1, 1].bar(comparison_df['Model'], comparison_df['Test F1-Score'])\n",
    "    axes[1, 1].set_title('Test F1-Score Comparison')\n",
    "    axes[1, 1].set_ylabel('F1-Score')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save comparison plot\n",
    "    comparison_plot_path = \"model_performance_comparison.png\"\n",
    "    plt.savefig(comparison_plot_path, dpi=300, bbox_inches='tight')\n",
    "    mlflow.log_artifact(comparison_plot_path)\n",
    "    plt.show()\n",
    "    \n",
    "    # Select best model based on ROC-AUC\n",
    "    best_model_name = comparison_df.loc[comparison_df['Test ROC-AUC'].idxmax(), 'Model']\n",
    "    best_model_metrics = model_results[best_model_name]\n",
    "    \n",
    "    # Log best model information\n",
    "    mlflow.log_param(\"best_model\", best_model_name)\n",
    "    mlflow.log_metric(\"best_model_accuracy\", best_model_metrics['test_accuracy'])\n",
    "    mlflow.log_metric(\"best_model_roc_auc\", best_model_metrics['test_roc_auc'])\n",
    "    mlflow.log_param(\"best_model_run_id\", best_model_metrics['run_id'])\n",
    "    \n",
    "    print(f\"\\nBest Model: {best_model_name}\")\n",
    "    print(f\"Best Model ROC-AUC: {best_model_metrics['test_roc_auc']:.4f}\")\n",
    "    print(f\"Best Model Accuracy: {best_model_metrics['test_accuracy']:.4f}\")\n",
    "    \n",
    "    # Log comparison metrics\n",
    "    mlflow.log_metric(\"models_compared\", len(model_results))\n",
    "    mlflow.log_metric(\"max_roc_auc\", comparison_df['Test ROC-AUC'].max())\n",
    "    mlflow.log_metric(\"min_roc_auc\", comparison_df['Test ROC-AUC'].min())\n",
    "    mlflow.log_metric(\"roc_auc_range\", comparison_df['Test ROC-AUC'].max() - comparison_df['Test ROC-AUC'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88194ddf",
   "metadata": {},
   "source": [
    "## 7. Model Registration and Deployment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a30e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the best model\n",
    "model_name_registry = \"customer_churn_predictor\"\n",
    "\n",
    "# Get the best model run\n",
    "best_run_id = best_model_metrics['run_id']\n",
    "model_uri = f\"runs:/{best_run_id}/model\"\n",
    "\n",
    "try:\n",
    "    # Register model\n",
    "    model_version = mlflow.register_model(\n",
    "        model_uri=model_uri,\n",
    "        name=model_name_registry,\n",
    "        description=f\"Best performing model ({best_model_name}) for customer churn prediction\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Model registered: {model_name_registry}\")\n",
    "    print(f\"Model version: {model_version.version}\")\n",
    "    print(f\"Model run ID: {best_run_id}\")\n",
    "    \n",
    "    # Transition model to Staging\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name_registry,\n",
    "        version=model_version.version,\n",
    "        stage=\"Staging\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Model transitioned to Staging stage\")\n",
    "    \n",
    "    # Add model description and tags\n",
    "    client.update_model_version(\n",
    "        name=model_name_registry,\n",
    "        version=model_version.version,\n",
    "        description=f\"\"\"\n",
    "        Best performing customer churn prediction model.\n",
    "        Model Type: {best_model_name}\n",
    "        ROC-AUC: {best_model_metrics['test_roc_auc']:.4f}\n",
    "        Accuracy: {best_model_metrics['test_accuracy']:.4f}\n",
    "        Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Set tags\n",
    "    client.set_model_version_tag(\n",
    "        name=model_name_registry,\n",
    "        version=model_version.version,\n",
    "        key=\"model_type\",\n",
    "        value=best_model_name\n",
    "    )\n",
    "    \n",
    "    client.set_model_version_tag(\n",
    "        name=model_name_registry,\n",
    "        version=model_version.version,\n",
    "        key=\"validation_status\",\n",
    "        value=\"validated\"\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Model registration failed: {e}\")\n",
    "    print(\"This might be due to MLflow server configuration. Model tracking still works.\")\n",
    "\n",
    "# Create deployment preparation summary\n",
    "deployment_info = {\n",
    "    \"model_name\": best_model_name,\n",
    "    \"model_uri\": model_uri,\n",
    "    \"run_id\": best_run_id,\n",
    "    \"accuracy\": best_model_metrics['test_accuracy'],\n",
    "    \"roc_auc\": best_model_metrics['test_roc_auc'],\n",
    "    \"precision\": best_model_metrics['test_precision'],\n",
    "    \"recall\": best_model_metrics['test_recall'],\n",
    "    \"f1_score\": best_model_metrics['test_f1']\n",
    "}\n",
    "\n",
    "# Save deployment info\n",
    "import json\n",
    "with open(\"deployment_info.json\", \"w\") as f:\n",
    "    json.dump(deployment_info, f, indent=2)\n",
    "\n",
    "print(\"\\nDeployment Information:\")\n",
    "for key, value in deployment_info.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1bbcc1",
   "metadata": {},
   "source": [
    "## 8. Experiment Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1452a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive experiment summary\n",
    "with mlflow.start_run(run_name=\"experiment_summary\") as run:\n",
    "    \n",
    "    # Get experiment information\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    experiment_id = experiment.experiment_id\n",
    "    \n",
    "    # Get all runs from the experiment\n",
    "    runs = mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "    \n",
    "    print(f\"Experiment Analysis for: {experiment_name}\")\n",
    "    print(f\"Total runs: {len(runs)}\")\n",
    "    print(f\"Experiment ID: {experiment_id}\")\n",
    "    \n",
    "    # Create experiment timeline\n",
    "    model_runs = runs[runs['tags.mlflow.runName'].str.contains('model_', na=False)]\n",
    "    \n",
    "    if not model_runs.empty:\n",
    "        # Performance over time plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Extract model names and metrics\n",
    "        model_names = []\n",
    "        accuracies = []\n",
    "        roc_aucs = []\n",
    "        \n",
    "        for _, run_row in model_runs.iterrows():\n",
    "            run_name = run_row['tags.mlflow.runName']\n",
    "            model_name = run_name.replace('model_', '').replace('_', ' ').title()\n",
    "            model_names.append(model_name)\n",
    "            accuracies.append(run_row.get('metrics.test_accuracy', 0))\n",
    "            roc_aucs.append(run_row.get('metrics.test_roc_auc', 0))\n",
    "        \n",
    "        # Create performance dashboard\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle(f'Experiment Dashboard: {experiment_name}', fontsize=16)\n",
    "        \n",
    "        # Model performance radar chart preparation\n",
    "        metrics_for_radar = ['test_accuracy', 'test_roc_auc', 'test_precision', 'test_recall', 'test_f1_score']\n",
    "        \n",
    "        # Performance comparison\n",
    "        x_pos = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[0, 0].bar(x_pos - width/2, accuracies, width, label='Accuracy', alpha=0.8)\n",
    "        axes[0, 0].bar(x_pos + width/2, roc_aucs, width, label='ROC-AUC', alpha=0.8)\n",
    "        axes[0, 0].set_xlabel('Models')\n",
    "        axes[0, 0].set_ylabel('Score')\n",
    "        axes[0, 0].set_title('Model Performance Comparison')\n",
    "        axes[0, 0].set_xticks(x_pos)\n",
    "        axes[0, 0].set_xticklabels(model_names, rotation=45)\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # ROC-AUC distribution\n",
    "        axes[0, 1].hist(roc_aucs, bins=min(5, len(roc_aucs)), alpha=0.7, edgecolor='black')\n",
    "        axes[0, 1].axvline(np.mean(roc_aucs), color='red', linestyle='--', label=f'Mean: {np.mean(roc_aucs):.3f}')\n",
    "        axes[0, 1].set_xlabel('ROC-AUC Score')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].set_title('ROC-AUC Score Distribution')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Training time comparison (if available)\n",
    "        training_times = []\n",
    "        for _, run_row in model_runs.iterrows():\n",
    "            training_time = run_row.get('metrics.training_time_seconds', 0)\n",
    "            training_times.append(training_time)\n",
    "        \n",
    "        if any(t > 0 for t in training_times):\n",
    "            axes[1, 0].bar(model_names, training_times)\n",
    "            axes[1, 0].set_xlabel('Models')\n",
    "            axes[1, 0].set_ylabel('Training Time (seconds)')\n",
    "            axes[1, 0].set_title('Training Time Comparison')\n",
    "            axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'Training time\\ndata not available', \n",
    "                           ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "            axes[1, 0].set_title('Training Time Comparison')\n",
    "        \n",
    "        # Model efficiency scatter plot (Performance vs Training Time)\n",
    "        if any(t > 0 for t in training_times):\n",
    "            scatter = axes[1, 1].scatter(training_times, roc_aucs, s=100, alpha=0.7)\n",
    "            for i, model in enumerate(model_names):\n",
    "                axes[1, 1].annotate(model, (training_times[i], roc_aucs[i]), \n",
    "                                   xytext=(5, 5), textcoords='offset points')\n",
    "            axes[1, 1].set_xlabel('Training Time (seconds)')\n",
    "            axes[1, 1].set_ylabel('ROC-AUC Score')\n",
    "            axes[1, 1].set_title('Model Efficiency (Performance vs Time)')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            # Show performance ranking instead\n",
    "            performance_rank = list(range(1, len(model_names) + 1))\n",
    "            sorted_indices = np.argsort(roc_aucs)[::-1]\n",
    "            \n",
    "            axes[1, 1].bar([model_names[i] for i in sorted_indices], \n",
    "                          [roc_aucs[i] for i in sorted_indices])\n",
    "            axes[1, 1].set_xlabel('Models (Ranked by Performance)')\n",
    "            axes[1, 1].set_ylabel('ROC-AUC Score')\n",
    "            axes[1, 1].set_title('Model Performance Ranking')\n",
    "            axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save experiment dashboard\n",
    "        dashboard_path = \"experiment_dashboard.png\"\n",
    "        plt.savefig(dashboard_path, dpi=300, bbox_inches='tight')\n",
    "        mlflow.log_artifact(dashboard_path)\n",
    "        plt.show()\n",
    "    \n",
    "    # Log experiment summary metrics\n",
    "    mlflow.log_metric(\"total_runs_in_experiment\", len(runs))\n",
    "    mlflow.log_metric(\"total_model_runs\", len(model_runs) if not model_runs.empty else 0)\n",
    "    \n",
    "    if not model_runs.empty:\n",
    "        mlflow.log_metric(\"best_roc_auc_overall\", max(roc_aucs))\n",
    "        mlflow.log_metric(\"avg_roc_auc_overall\", np.mean(roc_aucs))\n",
    "        mlflow.log_metric(\"std_roc_auc_overall\", np.std(roc_aucs))\n",
    "    \n",
    "    # Create final experiment report\n",
    "    report = f\"\"\"\n",
    "    EXPERIMENT SUMMARY REPORT\n",
    "    ========================\n",
    "    \n",
    "    Experiment: {experiment_name}\n",
    "    Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "    \n",
    "    Dataset Information:\n",
    "    - Total samples: {len(df)}\n",
    "    - Features after preprocessing: {len(feature_names)}\n",
    "    - Churn rate: {(y.sum() / len(y) * 100):.2f}%\n",
    "    \n",
    "    Models Evaluated: {len(model_results)}\n",
    "    \n",
    "    Best Model: {best_model_name}\n",
    "    - ROC-AUC: {best_model_metrics['test_roc_auc']:.4f}\n",
    "    - Accuracy: {best_model_metrics['test_accuracy']:.4f}\n",
    "    - Precision: {best_model_metrics['test_precision']:.4f}\n",
    "    - Recall: {best_model_metrics['test_recall']:.4f}\n",
    "    - F1-Score: {best_model_metrics['test_f1']:.4f}\n",
    "    \n",
    "    Model Performance Range:\n",
    "    - Best ROC-AUC: {max(roc_aucs) if roc_aucs else 'N/A':.4f}\n",
    "    - Worst ROC-AUC: {min(roc_aucs) if roc_aucs else 'N/A':.4f}\n",
    "    - Average ROC-AUC: {np.mean(roc_aucs) if roc_aucs else 'N/A':.4f}\n",
    "    \n",
    "    Next Steps:\n",
    "    1. Review model performance in MLflow UI\n",
    "    2. Deploy best model to staging environment\n",
    "    3. Set up monitoring and feedback loops\n",
    "    4. Schedule model retraining\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save report\n",
    "    with open(\"experiment_report.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    mlflow.log_artifact(\"experiment_report.txt\")\n",
    "    \n",
    "    print(report)\n",
    "    \n",
    "    print(f\"\\nüéâ Experiment completed successfully!\")\n",
    "    print(f\"\\nüìä View results in MLflow UI:\")\n",
    "    print(f\"   Run: mlflow ui --backend-store-uri ./mlruns\")\n",
    "    print(f\"   Then open: http://localhost:5000\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Artifacts saved:\")\n",
    "    print(f\"   - Model artifacts in MLflow\")\n",
    "    print(f\"   - Preprocessor: preprocessor.joblib\")\n",
    "    print(f\"   - Experiment report: experiment_report.txt\")\n",
    "    print(f\"   - Deployment info: deployment_info.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76944e49",
   "metadata": {},
   "source": [
    "## MLflow UI Instructions\n",
    "\n",
    "To view your experiment results:\n",
    "\n",
    "1. **Start MLflow UI**:\n",
    "   ```bash\n",
    "   mlflow ui --backend-store-uri ./mlruns\n",
    "   ```\n",
    "\n",
    "2. **Open browser**: Navigate to `http://localhost:5000`\n",
    "\n",
    "3. **Explore experiments**:\n",
    "   - Compare model performance\n",
    "   - View artifacts and plots\n",
    "   - Check model registry\n",
    "   - Download models and preprocessors\n",
    "\n",
    "4. **Model deployment**:\n",
    "   ```python\n",
    "   # Load model for inference\n",
    "   model = mlflow.sklearn.load_model(f\"runs:/{best_run_id}/model\")\n",
    "   \n",
    "   # Load preprocessor\n",
    "   import joblib\n",
    "   preprocessor = joblib.load(\"preprocessor.joblib\")\n",
    "   ```\n",
    "\n",
    "## Key Features Implemented:\n",
    "\n",
    "‚úÖ **Comprehensive Experiment Tracking**\n",
    "‚úÖ **Automated Model Comparison**\n",
    "‚úÖ **Artifact Management**\n",
    "‚úÖ **Model Registry Integration**\n",
    "‚úÖ **Performance Visualization**\n",
    "‚úÖ **Reproducible Experiments**\n",
    "‚úÖ **Deployment Preparation**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
