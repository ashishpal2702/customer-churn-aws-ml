{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4a5370a",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction with MLflow Experiment Tracking\n",
    "\n",
    "This notebook demonstrates end-to-end machine learning experiment tracking using MLflow for customer churn prediction.\n",
    "\n",
    "## Features:\n",
    "- Comprehensive experiment tracking\n",
    "- Model comparison and selection\n",
    "- Automated model registry\n",
    "- Reproducible experiments\n",
    "- Detailed performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcf1abf",
   "metadata": {},
   "source": [
    "## 1. Setup MLflow Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MLflow and dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"Successfully installed {package}\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"Failed to install {package}\")\n",
    "\n",
    "# Install required packages\n",
    "packages = ['mlflow', 'xgboost', 'scikit-learn', 'pandas', 'numpy', 'matplotlib', 'seaborn']\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"‚úì {package} is available\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6667a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration parameters\n",
    "USE_AWS_MLFLOW = False  # Set to True to use AWS SageMaker MLflow\n",
    "MLFLOW_TRACKING_ARN = \"arn:aws:sagemaker:us-east-1:{123456789}:mlflow-tracking-server/your-server-name\"  # Replace with your actual MLflow ARN\n",
    "\n",
    "# Configure MLflow tracking\n",
    "if USE_AWS_MLFLOW:\n",
    "    # AWS SageMaker MLflow tracking server\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_ARN)\n",
    "    print(f\"üìä Using AWS SageMaker MLflow tracking\")\n",
    "    print(f\"üîó Tracking ARN: {MLFLOW_TRACKING_ARN}\")\n",
    "else:\n",
    "    # Local MLflow tracking\n",
    "    mlflow.set_tracking_uri(\"./mlruns\")\n",
    "    print(f\"üè† Using local MLflow tracking\")\n",
    "    print(f\"üìÅ Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "# Create experiment\n",
    "experiment_name = f\"customer_churn_automl_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "try:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print(f\"‚úÖ Experiment created: {experiment_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Experiment creation issue: {e}\")\n",
    "    print(\"Continuing with default experiment...\")\n",
    "\n",
    "print(f\"üî¢ MLflow Version: {mlflow.__version__}\")\n",
    "print(f\"üéØ Experiment Focus: AutoML model metrics tracking only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b291dc",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8d55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data for AutoML\n",
    "print(\"üöÄ Loading and preparing data for AutoML...\")\n",
    "\n",
    "# Load dataset\n",
    "data_path = \"./data/customer_churn.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"üìä Dataset shape: {df.shape}\")\n",
    "print(f\"üéØ Target column: {'Churn' if 'Churn' in df.columns else 'Not found'}\")\n",
    "\n",
    "# Basic data info (no MLflow logging for data exploration)\n",
    "churn_rate = (df['Churn'].sum() / len(df)) * 100\n",
    "print(f\"üìà Churn rate: {churn_rate:.2f}%\")\n",
    "print(f\"üî¢ Total samples: {len(df)}\")\n",
    "print(f\"üìã Features: {len(df.columns) - 1}\")  # Excluding target\n",
    "\n",
    "# Quick data quality check\n",
    "missing_values = df.isnull().sum().sum()\n",
    "if missing_values > 0:\n",
    "    print(f\"‚ö†Ô∏è Missing values found: {missing_values}\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing values detected\")\n",
    "\n",
    "print(\"üì¶ Data ready for AutoML processing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea50e78",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54b4b9da",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing Pipeline with Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def prepare_data_for_automl(df):\n",
    "    \"\"\"Streamlined data preparation for AutoML\"\"\"\n",
    "    print(\"üîÑ Preparing data for AutoML...\")\n",
    "    \n",
    "    # Make a copy\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    if processed_df.isnull().sum().sum() > 0:\n",
    "        processed_df = processed_df.dropna()\n",
    "        print(f\"üßπ Dropped rows with missing values\")\n",
    "    \n",
    "    # Remove CustomerID if present\n",
    "    if 'CustomerID' in processed_df.columns:\n",
    "        processed_df = processed_df.drop('CustomerID', axis=1)\n",
    "    \n",
    "    # Define feature types\n",
    "    numerical_cols = processed_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'Churn' in numerical_cols:\n",
    "        numerical_cols.remove('Churn')\n",
    "    \n",
    "    categorical_cols = processed_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    print(f\"üìä Numerical features: {len(numerical_cols)}\")\n",
    "    print(f\"üìä Categorical features: {len(categorical_cols)}\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = processed_df.drop('Churn', axis=1)\n",
    "    y = processed_df['Churn']\n",
    "    \n",
    "    # Create simple preprocessing pipeline for AutoML\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_cols),\n",
    "            ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Fit and transform\n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # Get feature names for interpretability\n",
    "    try:\n",
    "        cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
    "        all_feature_names = numerical_cols + list(cat_feature_names)\n",
    "    except:\n",
    "        all_feature_names = [f\"feature_{i}\" for i in range(X_processed.shape[1])]\n",
    "    \n",
    "    print(f\"‚úÖ Data preprocessing completed\")\n",
    "    print(f\"üìà Final feature count: {X_processed.shape[1]}\")\n",
    "    \n",
    "    return X_processed, y, preprocessor, all_feature_names\n",
    "\n",
    "# Prepare data\n",
    "X_processed, y, preprocessor, feature_names = prepare_data_for_automl(df)\n",
    "\n",
    "# Split data for AutoML\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üèãÔ∏è Training set: {X_train.shape}\")\n",
    "print(f\"üß™ Test set: {X_test.shape}\")\n",
    "print(\"üéØ Ready for AutoML model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca02c245",
   "metadata": {},
   "source": [
    "## 4. Model Training with MLflow Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c21d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "# Try to import additional AutoML libraries\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except ImportError:\n",
    "    xgb_available = False\n",
    "    print(\"‚ö†Ô∏è XGBoost not available\")\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    lgb_available = True\n",
    "except ImportError:\n",
    "    lgb_available = False\n",
    "    print(\"‚ö†Ô∏è LightGBM not available\")\n",
    "\n",
    "print(\"ü§ñ Starting AutoML Model Training with MLflow Tracking\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define AutoML model suite\n",
    "automl_models = {\n",
    "    'Random_Forest': {\n",
    "        'model': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "        'params': {'n_estimators': 100, 'max_depth': None, 'random_state': 42}\n",
    "    },\n",
    "    'Gradient_Boosting': {\n",
    "        'model': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "        'params': {'n_estimators': 100, 'learning_rate': 0.1, 'random_state': 42}\n",
    "    },\n",
    "    'Logistic_Regression': {\n",
    "        'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'params': {'C': 1.0, 'max_iter': 1000, 'random_state': 42}\n",
    "    },\n",
    "    'SVM_RBF': {\n",
    "        'model': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "        'params': {'kernel': 'rbf', 'C': 1.0, 'random_state': 42}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add optional models if available\n",
    "if xgb_available:\n",
    "    automl_models['XGBoost'] = {\n",
    "        'model': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        'params': {'n_estimators': 100, 'max_depth': 6, 'random_state': 42}\n",
    "    }\n",
    "\n",
    "if lgb_available:\n",
    "    automl_models['LightGBM'] = {\n",
    "        'model': LGBMClassifier(random_state=42, verbose=-1),\n",
    "        'params': {'n_estimators': 100, 'num_leaves': 31, 'random_state': 42}\n",
    "    }\n",
    "\n",
    "print(f\"üéØ AutoML will evaluate {len(automl_models)} models\")\n",
    "\n",
    "# AutoML Training Loop with MLflow\n",
    "model_results = {}\n",
    "\n",
    "for model_name, config in automl_models.items():\n",
    "    with mlflow.start_run(run_name=f\"automl_{model_name}\") as run:\n",
    "        print(f\"\\nüî• Training: {model_name}\")\n",
    "        \n",
    "        # Log only essential model parameters\n",
    "        mlflow.log_param(\"model_algorithm\", model_name)\n",
    "        mlflow.log_param(\"automl_mode\", True)\n",
    "        \n",
    "        model = config['model']\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Cross-validation for robust evaluation\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "        \n",
    "        # Train final model\n",
    "        model.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Generate predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        metrics = {\n",
    "            'test_accuracy': accuracy_score(y_test, y_pred),\n",
    "            'test_roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "            'test_precision': precision_score(y_test, y_pred),\n",
    "            'test_recall': recall_score(y_test, y_pred),\n",
    "            'test_f1_score': f1_score(y_test, y_pred),\n",
    "            'cv_roc_auc_mean': cv_scores.mean(),\n",
    "            'cv_roc_auc_std': cv_scores.std(),\n",
    "            'training_time_seconds': training_time\n",
    "        }\n",
    "        \n",
    "        # Log metrics to MLflow\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "        \n",
    "        # Log model to MLflow\n",
    "        if model_name == 'XGBoost' and xgb_available:\n",
    "            mlflow.xgboost.log_model(model, \"model\")\n",
    "        elif model_name == 'LightGBM' and lgb_available:\n",
    "            mlflow.lightgbm.log_model(model, \"model\")\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        # Store results for comparison\n",
    "        model_results[model_name] = {\n",
    "            'model': model,\n",
    "            'metrics': metrics,\n",
    "            'run_id': run.info.run_id\n",
    "        }\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"  ‚úÖ ROC-AUC: {metrics['test_roc_auc']:.4f}\")\n",
    "        print(f\"  üìä Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "        print(f\"  ‚è±Ô∏è Time: {training_time:.2f}s\")\n",
    "\n",
    "print(f\"\\nüéâ AutoML Training Complete! Evaluated {len(model_results)} models\")\n",
    "print(\"üìä All model metrics logged to MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c47175",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoML Model Comparison and Selection\n",
    "with mlflow.start_run(run_name=\"automl_comparison\") as run:\n",
    "    \n",
    "    print(\"üèÜ AutoML Model Comparison Results\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for model_name, results in model_results.items():\n",
    "        metrics = results['metrics']\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'ROC-AUC': metrics['test_roc_auc'],\n",
    "            'Accuracy': metrics['test_accuracy'],\n",
    "            'Precision': metrics['test_precision'],\n",
    "            'Recall': metrics['test_recall'],\n",
    "            'F1-Score': metrics['test_f1_score'],\n",
    "            'CV_ROC_AUC': metrics['cv_roc_auc_mean'],\n",
    "            'Training_Time': metrics['training_time_seconds']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.sort_values('ROC-AUC', ascending=False)\n",
    "    \n",
    "    print(\"\\nüìä Model Performance Ranking:\")\n",
    "    print(comparison_df.round(4))\n",
    "    \n",
    "    # Select best model\n",
    "    best_model_name = comparison_df.iloc[0]['Model']\n",
    "    best_model_metrics = model_results[best_model_name]['metrics']\n",
    "    best_run_id = model_results[best_model_name]['run_id']\n",
    "    \n",
    "    # Log comparison metrics to MLflow\n",
    "    mlflow.log_param(\"automl_models_compared\", len(model_results))\n",
    "    mlflow.log_param(\"best_model_algorithm\", best_model_name)\n",
    "    mlflow.log_metric(\"best_roc_auc\", best_model_metrics['test_roc_auc'])\n",
    "    mlflow.log_metric(\"best_accuracy\", best_model_metrics['test_accuracy'])\n",
    "    mlflow.log_metric(\"models_evaluated\", len(model_results))\n",
    "    \n",
    "    # Create simple performance visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # ROC-AUC comparison\n",
    "    plt.subplot(1, 2, 1)\n",
    "    bars = plt.bar(comparison_df['Model'], comparison_df['ROC-AUC'], color='skyblue', alpha=0.7)\n",
    "    plt.title('AutoML Model Performance (ROC-AUC)', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('ROC-AUC Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Highlight best model\n",
    "    bars[0].set_color('gold')\n",
    "    bars[0].set_alpha(1.0)\n",
    "    \n",
    "    # Training time comparison\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(comparison_df['Model'], comparison_df['Training_Time'], color='lightcoral', alpha=0.7)\n",
    "    plt.title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Training Time (seconds)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save and log visualization\n",
    "    plot_path = \"automl_comparison.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    mlflow.log_artifact(plot_path)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nü•á Best AutoML Model: {best_model_name}\")\n",
    "    print(f\"   üéØ ROC-AUC: {best_model_metrics['test_roc_auc']:.4f}\")\n",
    "    print(f\"   üìä Accuracy: {best_model_metrics['test_accuracy']:.4f}\")\n",
    "    print(f\"   üé≤ Precision: {best_model_metrics['test_precision']:.4f}\")\n",
    "    print(f\"   üìà Recall: {best_model_metrics['test_recall']:.4f}\")\n",
    "    print(f\"   ‚öñÔ∏è F1-Score: {best_model_metrics['test_f1_score']:.4f}\")\n",
    "    print(f\"   ‚è±Ô∏è Training Time: {best_model_metrics['training_time_seconds']:.2f}s\")\n",
    "    \n",
    "    print(f\"\\nüìã AutoML Summary:\")\n",
    "    print(f\"   ‚Ä¢ Models Evaluated: {len(model_results)}\")\n",
    "    print(f\"   ‚Ä¢ Best Performance: {best_model_metrics['test_roc_auc']:.4f} ROC-AUC\")\n",
    "    print(f\"   ‚Ä¢ Performance Range: {comparison_df['ROC-AUC'].min():.4f} - {comparison_df['ROC-AUC'].max():.4f}\")\n",
    "    \n",
    "    # Store best model info for next steps\n",
    "    best_model_info = {\n",
    "        'name': best_model_name,\n",
    "        'run_id': best_run_id,\n",
    "        'metrics': best_model_metrics,\n",
    "        'model_uri': f\"runs:/{best_run_id}/model\"\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
