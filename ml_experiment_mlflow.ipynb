{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed38fe21",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction with MLflow Experiment Tracking\n",
    "\n",
    "This notebook demonstrates end-to-end machine learning experiment tracking using MLflow for customer churn prediction.\n",
    "\n",
    "## Features:\n",
    "- Comprehensive experiment tracking\n",
    "- Model comparison and selection\n",
    "- Automated model registry\n",
    "- Reproducible experiments\n",
    "- Detailed performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2380cd",
   "metadata": {},
   "source": [
    "## 1. Setup MLflow Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955bfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MLflow and dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"Successfully installed {package}\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"Failed to install {package}\")\n",
    "\n",
    "# Install required packages\n",
    "packages = ['mlflow', 'xgboost', 'scikit-learn', 'pandas', 'numpy', 'matplotlib', 'seaborn']\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"‚úì {package} is available\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec07084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration parameters\n",
    "USE_AWS_MLFLOW = False  # Set to True to use AWS SageMaker MLflow\n",
    "MLFLOW_TRACKING_ARN = \"arn:aws:sagemaker:us-east-1:{123456789}:mlflow-tracking-server/your-server-name\"  # Replace with your actual MLflow ARN\n",
    "\n",
    "# Configure MLflow tracking\n",
    "if USE_AWS_MLFLOW:\n",
    "    # AWS SageMaker MLflow tracking server\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_ARN)\n",
    "    print(f\"üìä Using AWS SageMaker MLflow tracking\")\n",
    "    print(f\"üîó Tracking ARN: {MLFLOW_TRACKING_ARN}\")\n",
    "else:\n",
    "    # Local MLflow tracking\n",
    "    mlflow.set_tracking_uri(\"./mlruns\")\n",
    "    print(f\"üè† Using local MLflow tracking\")\n",
    "    print(f\"üìÅ Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "# Create experiment\n",
    "experiment_name = f\"customer_churn_automl_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "try:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print(f\"‚úÖ Experiment created: {experiment_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Experiment creation issue: {e}\")\n",
    "    print(\"Continuing with default experiment...\")\n",
    "\n",
    "print(f\"üî¢ MLflow Version: {mlflow.__version__}\")\n",
    "print(f\"üéØ Experiment Focus: AutoML model metrics tracking only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daee5e4",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data for AutoML\n",
    "print(\"üöÄ Loading and preparing data for AutoML...\")\n",
    "\n",
    "# Load dataset\n",
    "data_path = \"./data/customer_churn.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"üìä Dataset shape: {df.shape}\")\n",
    "print(f\"üéØ Target column: {'Churn' if 'Churn' in df.columns else 'Not found'}\")\n",
    "\n",
    "# Basic data info (no MLflow logging for data exploration)\n",
    "churn_rate = (df['Churn'].sum() / len(df)) * 100\n",
    "print(f\"üìà Churn rate: {churn_rate:.2f}%\")\n",
    "print(f\"üî¢ Total samples: {len(df)}\")\n",
    "print(f\"üìã Features: {len(df.columns) - 1}\")  # Excluding target\n",
    "\n",
    "# Quick data quality check\n",
    "missing_values = df.isnull().sum().sum()\n",
    "if missing_values > 0:\n",
    "    print(f\"‚ö†Ô∏è Missing values found: {missing_values}\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing values detected\")\n",
    "\n",
    "print(\"üì¶ Data ready for AutoML processing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89643b",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis with MLflow Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28df3a2b",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing Pipeline with Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71d83ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def prepare_data_for_automl(df):\n",
    "    \"\"\"Streamlined data preparation for AutoML\"\"\"\n",
    "    print(\"üîÑ Preparing data for AutoML...\")\n",
    "    \n",
    "    # Make a copy\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    if processed_df.isnull().sum().sum() > 0:\n",
    "        processed_df = processed_df.dropna()\n",
    "        print(f\"üßπ Dropped rows with missing values\")\n",
    "    \n",
    "    # Remove CustomerID if present\n",
    "    if 'CustomerID' in processed_df.columns:\n",
    "        processed_df = processed_df.drop('CustomerID', axis=1)\n",
    "    \n",
    "    # Define feature types\n",
    "    numerical_cols = processed_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'Churn' in numerical_cols:\n",
    "        numerical_cols.remove('Churn')\n",
    "    \n",
    "    categorical_cols = processed_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    print(f\"üìä Numerical features: {len(numerical_cols)}\")\n",
    "    print(f\"üìä Categorical features: {len(categorical_cols)}\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = processed_df.drop('Churn', axis=1)\n",
    "    y = processed_df['Churn']\n",
    "    \n",
    "    # Create simple preprocessing pipeline for AutoML\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_cols),\n",
    "            ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Fit and transform\n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # Get feature names for interpretability\n",
    "    try:\n",
    "        cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
    "        all_feature_names = numerical_cols + list(cat_feature_names)\n",
    "    except:\n",
    "        all_feature_names = [f\"feature_{i}\" for i in range(X_processed.shape[1])]\n",
    "    \n",
    "    print(f\"‚úÖ Data preprocessing completed\")\n",
    "    print(f\"üìà Final feature count: {X_processed.shape[1]}\")\n",
    "    \n",
    "    return X_processed, y, preprocessor, all_feature_names\n",
    "\n",
    "# Prepare data\n",
    "X_processed, y, preprocessor, feature_names = prepare_data_for_automl(df)\n",
    "\n",
    "# Split data for AutoML\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üèãÔ∏è Training set: {X_train.shape}\")\n",
    "print(f\"üß™ Test set: {X_test.shape}\")\n",
    "print(\"üéØ Ready for AutoML model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020e30aa",
   "metadata": {},
   "source": [
    "## 5. Model Training with MLflow Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd902181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "# Try to import additional AutoML libraries\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except ImportError:\n",
    "    xgb_available = False\n",
    "    print(\"‚ö†Ô∏è XGBoost not available\")\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    lgb_available = True\n",
    "except ImportError:\n",
    "    lgb_available = False\n",
    "    print(\"‚ö†Ô∏è LightGBM not available\")\n",
    "\n",
    "print(\"ü§ñ Starting AutoML Model Training with MLflow Tracking\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define AutoML model suite\n",
    "automl_models = {\n",
    "    'Random_Forest': {\n",
    "        'model': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "        'params': {'n_estimators': 100, 'max_depth': None, 'random_state': 42}\n",
    "    },\n",
    "    'Gradient_Boosting': {\n",
    "        'model': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "        'params': {'n_estimators': 100, 'learning_rate': 0.1, 'random_state': 42}\n",
    "    },\n",
    "    'Logistic_Regression': {\n",
    "        'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'params': {'C': 1.0, 'max_iter': 1000, 'random_state': 42}\n",
    "    },\n",
    "    'SVM_RBF': {\n",
    "        'model': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "        'params': {'kernel': 'rbf', 'C': 1.0, 'random_state': 42}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add optional models if available\n",
    "if xgb_available:\n",
    "    automl_models['XGBoost'] = {\n",
    "        'model': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        'params': {'n_estimators': 100, 'max_depth': 6, 'random_state': 42}\n",
    "    }\n",
    "\n",
    "if lgb_available:\n",
    "    automl_models['LightGBM'] = {\n",
    "        'model': LGBMClassifier(random_state=42, verbose=-1),\n",
    "        'params': {'n_estimators': 100, 'num_leaves': 31, 'random_state': 42}\n",
    "    }\n",
    "\n",
    "print(f\"üéØ AutoML will evaluate {len(automl_models)} models\")\n",
    "\n",
    "# AutoML Training Loop with MLflow\n",
    "model_results = {}\n",
    "\n",
    "for model_name, config in automl_models.items():\n",
    "    with mlflow.start_run(run_name=f\"automl_{model_name}\") as run:\n",
    "        print(f\"\\nüî• Training: {model_name}\")\n",
    "        \n",
    "        # Log only essential model parameters\n",
    "        mlflow.log_param(\"model_algorithm\", model_name)\n",
    "        mlflow.log_param(\"automl_mode\", True)\n",
    "        \n",
    "        model = config['model']\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Cross-validation for robust evaluation\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "        \n",
    "        # Train final model\n",
    "        model.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Generate predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        metrics = {\n",
    "            'test_accuracy': accuracy_score(y_test, y_pred),\n",
    "            'test_roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "            'test_precision': precision_score(y_test, y_pred),\n",
    "            'test_recall': recall_score(y_test, y_pred),\n",
    "            'test_f1_score': f1_score(y_test, y_pred),\n",
    "            'cv_roc_auc_mean': cv_scores.mean(),\n",
    "            'cv_roc_auc_std': cv_scores.std(),\n",
    "            'training_time_seconds': training_time\n",
    "        }\n",
    "        \n",
    "        # Log metrics to MLflow\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "        \n",
    "        # Log model to MLflow\n",
    "        if model_name == 'XGBoost' and xgb_available:\n",
    "            mlflow.xgboost.log_model(model, \"model\")\n",
    "        elif model_name == 'LightGBM' and lgb_available:\n",
    "            mlflow.lightgbm.log_model(model, \"model\")\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        # Store results for comparison\n",
    "        model_results[model_name] = {\n",
    "            'model': model,\n",
    "            'metrics': metrics,\n",
    "            'run_id': run.info.run_id\n",
    "        }\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"  ‚úÖ ROC-AUC: {metrics['test_roc_auc']:.4f}\")\n",
    "        print(f\"  üìä Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "        print(f\"  ‚è±Ô∏è Time: {training_time:.2f}s\")\n",
    "\n",
    "print(f\"\\nüéâ AutoML Training Complete! Evaluated {len(model_results)} models\")\n",
    "print(\"üìä All model metrics logged to MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7b24af",
   "metadata": {},
   "source": [
    "## 6. Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoML Model Comparison and Selection\n",
    "with mlflow.start_run(run_name=\"automl_comparison\") as run:\n",
    "    \n",
    "    print(\"üèÜ AutoML Model Comparison Results\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for model_name, results in model_results.items():\n",
    "        metrics = results['metrics']\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'ROC-AUC': metrics['test_roc_auc'],\n",
    "            'Accuracy': metrics['test_accuracy'],\n",
    "            'Precision': metrics['test_precision'],\n",
    "            'Recall': metrics['test_recall'],\n",
    "            'F1-Score': metrics['test_f1_score'],\n",
    "            'CV_ROC_AUC': metrics['cv_roc_auc_mean'],\n",
    "            'Training_Time': metrics['training_time_seconds']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.sort_values('ROC-AUC', ascending=False)\n",
    "    \n",
    "    print(\"\\nüìä Model Performance Ranking:\")\n",
    "    print(comparison_df.round(4))\n",
    "    \n",
    "    # Select best model\n",
    "    best_model_name = comparison_df.iloc[0]['Model']\n",
    "    best_model_metrics = model_results[best_model_name]['metrics']\n",
    "    best_run_id = model_results[best_model_name]['run_id']\n",
    "    \n",
    "    # Log comparison metrics to MLflow\n",
    "    mlflow.log_param(\"automl_models_compared\", len(model_results))\n",
    "    mlflow.log_param(\"best_model_algorithm\", best_model_name)\n",
    "    mlflow.log_metric(\"best_roc_auc\", best_model_metrics['test_roc_auc'])\n",
    "    mlflow.log_metric(\"best_accuracy\", best_model_metrics['test_accuracy'])\n",
    "    mlflow.log_metric(\"models_evaluated\", len(model_results))\n",
    "    \n",
    "    # Create simple performance visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # ROC-AUC comparison\n",
    "    plt.subplot(1, 2, 1)\n",
    "    bars = plt.bar(comparison_df['Model'], comparison_df['ROC-AUC'], color='skyblue', alpha=0.7)\n",
    "    plt.title('AutoML Model Performance (ROC-AUC)', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('ROC-AUC Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Highlight best model\n",
    "    bars[0].set_color('gold')\n",
    "    bars[0].set_alpha(1.0)\n",
    "    \n",
    "    # Training time comparison\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(comparison_df['Model'], comparison_df['Training_Time'], color='lightcoral', alpha=0.7)\n",
    "    plt.title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Training Time (seconds)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save and log visualization\n",
    "    plot_path = \"automl_comparison.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    mlflow.log_artifact(plot_path)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nü•á Best AutoML Model: {best_model_name}\")\n",
    "    print(f\"   üéØ ROC-AUC: {best_model_metrics['test_roc_auc']:.4f}\")\n",
    "    print(f\"   üìä Accuracy: {best_model_metrics['test_accuracy']:.4f}\")\n",
    "    print(f\"   üé≤ Precision: {best_model_metrics['test_precision']:.4f}\")\n",
    "    print(f\"   üìà Recall: {best_model_metrics['test_recall']:.4f}\")\n",
    "    print(f\"   ‚öñÔ∏è F1-Score: {best_model_metrics['test_f1_score']:.4f}\")\n",
    "    print(f\"   ‚è±Ô∏è Training Time: {best_model_metrics['training_time_seconds']:.2f}s\")\n",
    "    \n",
    "    print(f\"\\nüìã AutoML Summary:\")\n",
    "    print(f\"   ‚Ä¢ Models Evaluated: {len(model_results)}\")\n",
    "    print(f\"   ‚Ä¢ Best Performance: {best_model_metrics['test_roc_auc']:.4f} ROC-AUC\")\n",
    "    print(f\"   ‚Ä¢ Performance Range: {comparison_df['ROC-AUC'].min():.4f} - {comparison_df['ROC-AUC'].max():.4f}\")\n",
    "    \n",
    "    # Store best model info for next steps\n",
    "    best_model_info = {\n",
    "        'name': best_model_name,\n",
    "        'run_id': best_run_id,\n",
    "        'metrics': best_model_metrics,\n",
    "        'model_uri': f\"runs:/{best_run_id}/model\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88194ddf",
   "metadata": {},
   "source": [
    "## 7. Model Registration and Deployment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a30e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoML Model Registration and Deployment Preparation\n",
    "print(\"üöÄ Registering Best AutoML Model\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Model registry configuration\n",
    "model_name_registry = \"customer-churn-automl\"\n",
    "model_uri = best_model_info['model_uri']\n",
    "best_model_name = best_model_info['name']\n",
    "best_run_id = best_model_info['run_id']\n",
    "best_metrics = best_model_info['metrics']\n",
    "\n",
    "try:\n",
    "    # Register the best AutoML model\n",
    "    model_version = mlflow.register_model(\n",
    "        model_uri=model_uri,\n",
    "        name=model_name_registry,\n",
    "        description=f\"AutoML best performing model ({best_model_name}) for customer churn prediction\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model registered successfully!\")\n",
    "    print(f\"   üìù Registry Name: {model_name_registry}\")\n",
    "    print(f\"   üî¢ Version: {model_version.version}\")\n",
    "    print(f\"   üÜî Run ID: {best_run_id}\")\n",
    "    print(f\"   ü§ñ Algorithm: {best_model_name}\")\n",
    "    \n",
    "    # Transition model to Staging\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name_registry,\n",
    "        version=model_version.version,\n",
    "        stage=\"Staging\"\n",
    "    )\n",
    "    \n",
    "    print(f\"   üìà Stage: Staging\")\n",
    "    \n",
    "    # Add metadata and tags\n",
    "    client.update_model_version(\n",
    "        name=model_name_registry,\n",
    "        version=model_version.version,\n",
    "        description=f\"\"\"\n",
    "        AutoML Selected Best Model for Customer Churn Prediction\n",
    "        \n",
    "        Algorithm: {best_model_name}\n",
    "        ROC-AUC: {best_metrics['test_roc_auc']:.4f}\n",
    "        Accuracy: {best_metrics['test_accuracy']:.4f}\n",
    "        Precision: {best_metrics['test_precision']:.4f}\n",
    "        Recall: {best_metrics['test_recall']:.4f}\n",
    "        F1-Score: {best_metrics['test_f1_score']:.4f}\n",
    "        \n",
    "        Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        AutoML Process: Evaluated {len(model_results)} algorithms\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Set informative tags\n",
    "    client.set_model_version_tag(model_name_registry, model_version.version, \"algorithm\", best_model_name)\n",
    "    client.set_model_version_tag(model_name_registry, model_version.version, \"automl\", \"true\")\n",
    "    client.set_model_version_tag(model_name_registry, model_version.version, \"validation_status\", \"validated\")\n",
    "    client.set_model_version_tag(model_name_registry, model_version.version, \"deployment_ready\", \"true\")\n",
    "    \n",
    "    print(f\"   üè∑Ô∏è Tags: Added AutoML and algorithm tags\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model registration failed: {e}\")\n",
    "    print(\"üí° Troubleshooting:\")\n",
    "    if \"pattern\" in str(e):\n",
    "        print(\"   ‚Ä¢ Model name contains invalid characters\")\n",
    "        print(\"   ‚Ä¢ Use only alphanumeric characters and hyphens\")\n",
    "    if USE_AWS_MLFLOW:\n",
    "        print(\"   ‚Ä¢ Check AWS SageMaker MLflow tracking server status\")\n",
    "        print(\"   ‚Ä¢ Verify ARN and permissions\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ Local MLflow registry may have limitations\")\n",
    "    \n",
    "    print(\"   ‚úÖ Model artifacts are still saved in run tracking\")\n",
    "\n",
    "# Create deployment information\n",
    "deployment_info = {\n",
    "    \"automl_best_model\": best_model_name,\n",
    "    \"model_registry_name\": model_name_registry,\n",
    "    \"model_uri\": model_uri,\n",
    "    \"run_id\": best_run_id,\n",
    "    \"performance_metrics\": {\n",
    "        \"roc_auc\": round(best_metrics['test_roc_auc'], 4),\n",
    "        \"accuracy\": round(best_metrics['test_accuracy'], 4),\n",
    "        \"precision\": round(best_metrics['test_precision'], 4),\n",
    "        \"recall\": round(best_metrics['test_recall'], 4),\n",
    "        \"f1_score\": round(best_metrics['test_f1_score'], 4)\n",
    "    },\n",
    "    \"automl_summary\": {\n",
    "        \"models_evaluated\": len(model_results),\n",
    "        \"tracking_uri\": mlflow.get_tracking_uri(),\n",
    "        \"experiment_name\": experiment_name\n",
    "    },\n",
    "    \"deployment_timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "# Save deployment information\n",
    "import json\n",
    "with open(\"automl_deployment_info.json\", \"w\") as f:\n",
    "    json.dump(deployment_info, f, indent=2)\n",
    "\n",
    "print(f\"\\nüìã AutoML Deployment Summary:\")\n",
    "print(f\"   ü§ñ Best Algorithm: {best_model_name}\")\n",
    "print(f\"   üéØ Performance: {best_metrics['test_roc_auc']:.4f} ROC-AUC\")\n",
    "print(f\"   üìä Models Compared: {len(model_results)}\")\n",
    "print(f\"   üíæ Info Saved: automl_deployment_info.json\")\n",
    "print(f\"   üìç Tracking: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "print(f\"\\nüéâ AutoML model registration completed!\")\n",
    "print(f\"üöÄ Ready for deployment to production!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1bbcc1",
   "metadata": {},
   "source": [
    "## 8. Experiment Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1452a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoML Model Artifacts & Deployment Readiness\n",
    "print(\"üì¶ Preparing AutoML Model Artifacts\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    # Load the best AutoML model for final validation\n",
    "    best_model_loaded = mlflow.pyfunc.load_model(best_model_info['model_uri'])\n",
    "    \n",
    "    print(f\"‚úÖ Best model loaded successfully\")\n",
    "    print(f\"   ü§ñ Algorithm: {best_model_info['name']}\")\n",
    "    print(f\"   üÜî Run ID: {best_model_info['run_id']}\")\n",
    "    \n",
    "    # Validate model functionality with sample prediction\n",
    "    sample_data = X_test.head(3)\n",
    "    sample_predictions = best_model_loaded.predict(sample_data)\n",
    "    \n",
    "    print(f\"   üß™ Sample Prediction Test:\")\n",
    "    print(f\"      ‚Ä¢ Input shape: {sample_data.shape}\")\n",
    "    print(f\"      ‚Ä¢ Predictions: {sample_predictions}\")\n",
    "    print(f\"      ‚Ä¢ Model callable: ‚úÖ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model loading failed: {e}\")\n",
    "    if \"No such file or directory\" in str(e):\n",
    "        print(\"üí° Model artifacts may not be saved correctly\")\n",
    "    else:\n",
    "        print(\"üí° Model format or dependency issue\")\n",
    "\n",
    "# Create comprehensive model documentation\n",
    "model_documentation = {\n",
    "    \"automl_experiment\": {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"tracking_uri\": mlflow.get_tracking_uri(),\n",
    "        \"aws_mlflow_enabled\": USE_AWS_MLFLOW,\n",
    "        \"total_models_evaluated\": len(model_results)\n",
    "    },\n",
    "    \"best_model\": {\n",
    "        \"algorithm\": best_model_info['name'],\n",
    "        \"run_id\": best_model_info['run_id'],\n",
    "        \"model_uri\": best_model_info['model_uri'],\n",
    "        \"performance_metrics\": best_model_info['metrics']\n",
    "    },\n",
    "    \"model_comparison_results\": [\n",
    "        {\n",
    "            \"algorithm\": result['name'],\n",
    "            \"run_id\": result['run_id'],\n",
    "            \"roc_auc\": round(result['metrics']['test_roc_auc'], 4),\n",
    "            \"accuracy\": round(result['metrics']['test_accuracy'], 4),\n",
    "            \"f1_score\": round(result['metrics']['test_f1_score'], 4),\n",
    "            \"ranking\": idx + 1\n",
    "        }\n",
    "        for idx, result in enumerate(model_results)\n",
    "    ],\n",
    "    \"data_info\": {\n",
    "        \"features_count\": X_train.shape[1],\n",
    "        \"training_samples\": X_train.shape[0],\n",
    "        \"test_samples\": X_test.shape[0],\n",
    "        \"feature_names\": X_train.columns.tolist(),\n",
    "        \"target_distribution\": {\n",
    "            \"churn_rate\": round(y_train.mean(), 3),\n",
    "            \"class_balance\": f\"{round((1-y_train.mean())*100, 1)}% No Churn, {round(y_train.mean()*100, 1)}% Churn\"\n",
    "        }\n",
    "    },\n",
    "    \"deployment_artifacts\": {\n",
    "        \"model_registry_name\": \"customer-churn-automl\",\n",
    "        \"preprocessing_available\": True,\n",
    "        \"local_model_file\": \"model/best_model_xgboost.joblib\",  # Will be updated based on best model\n",
    "        \"deployment_info_file\": \"automl_deployment_info.json\",\n",
    "        \"requirements_file\": \"requirements.txt\"\n",
    "    },\n",
    "    \"metadata\": {\n",
    "        \"created_timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"mlflow_version\": mlflow.__version__,\n",
    "        \"python_version\": f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\",\n",
    "        \"automl_approach\": \"comparative_evaluation\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save comprehensive documentation\n",
    "with open(\"automl_model_documentation.json\", \"w\") as f:\n",
    "    json.dump(model_documentation, f, indent=2)\n",
    "\n",
    "print(f\"\\nüìö Documentation Created:\")\n",
    "print(f\"   üìÑ automl_model_documentation.json - Complete experiment documentation\")\n",
    "print(f\"   üìÑ automl_deployment_info.json - Deployment-ready information\")\n",
    "\n",
    "# Save best model locally for traditional deployment\n",
    "try:\n",
    "    import joblib\n",
    "    \n",
    "    # Extract the underlying sklearn model if available\n",
    "    if hasattr(best_model_loaded, '_model_impl'):\n",
    "        sklearn_model = best_model_loaded._model_impl.sklearn_model\n",
    "    else:\n",
    "        sklearn_model = best_model_loaded\n",
    "    \n",
    "    # Update local model file based on best algorithm\n",
    "    best_algorithm = best_model_info['name']\n",
    "    local_model_filename = f\"model/best_model_{best_algorithm.lower().replace(' ', '_')}.joblib\"\n",
    "    \n",
    "    joblib.dump(sklearn_model, local_model_filename)\n",
    "    print(f\"   üíæ Local model saved: {local_model_filename}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Local model save warning: {e}\")\n",
    "    print(f\"   üí° Model available via MLflow URI: {best_model_info['model_uri']}\")\n",
    "\n",
    "# Final AutoML Summary\n",
    "print(f\"\\nüéØ AutoML Experiment Complete!\")\n",
    "print(f\"=\" * 40)\n",
    "print(f\"üèÜ Best Algorithm: {best_model_info['name']}\")\n",
    "print(f\"üìä ROC-AUC Score: {best_model_info['metrics']['test_roc_auc']:.4f}\")\n",
    "print(f\"üé™ Models Evaluated: {len(model_results)}\")\n",
    "print(f\"üìç MLflow Tracking: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "if USE_AWS_MLFLOW:\n",
    "    print(f\"‚òÅÔ∏è AWS SageMaker MLflow: Enabled\")\n",
    "    print(f\"üîó Tracking ARN: {MLFLOW_TRACKING_ARN[:50]}...\")\n",
    "else:\n",
    "    print(f\"\udfe0 Local MLflow: ./mlruns\")\n",
    "\n",
    "print(f\"\\n\ude80 Deployment Options:\")\n",
    "print(f\"   1. MLflow Model URI: {best_model_info['model_uri']}\")\n",
    "print(f\"   2. Local Model File: Available\")\n",
    "print(f\"   3. SageMaker Endpoint: Ready for deployment\")\n",
    "print(f\"   4. Model Registry: customer-churn-automl\")\n",
    "\n",
    "print(f\"\\n‚úÖ All AutoML artifacts prepared for production deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76944e49",
   "metadata": {},
   "source": [
    "## MLflow UI Instructions\n",
    "\n",
    "To view your experiment results:\n",
    "\n",
    "1. **Start MLflow UI**:\n",
    "   ```bash\n",
    "   mlflow ui --backend-store-uri ./mlruns\n",
    "   ```\n",
    "\n",
    "2. **Open browser**: Navigate to `http://localhost:5000`\n",
    "\n",
    "3. **Explore experiments**:\n",
    "   - Compare model performance\n",
    "   - View artifacts and plots\n",
    "   - Check model registry\n",
    "   - Download models and preprocessors\n",
    "\n",
    "4. **Model deployment**:\n",
    "   ```python\n",
    "   # Load model for inference\n",
    "   model = mlflow.sklearn.load_model(f\"runs:/{best_run_id}/model\")\n",
    "   \n",
    "   # Load preprocessor\n",
    "   import joblib\n",
    "   preprocessor = joblib.load(\"preprocessor.joblib\")\n",
    "   ```\n",
    "\n",
    "## Key Features Implemented:\n",
    "\n",
    "‚úÖ **Comprehensive Experiment Tracking**\n",
    "\n",
    "‚úÖ **Automated Model Comparison**\n",
    "\n",
    "‚úÖ **Artifact Management**\n",
    "\n",
    "‚úÖ **Model Registry Integration**\n",
    "\n",
    "‚úÖ **Performance Visualization**\n",
    "\n",
    "‚úÖ **Reproducible Experiments**\n",
    "\n",
    "‚úÖ **Deployment Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7d81fe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
